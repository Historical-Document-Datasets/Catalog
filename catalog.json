[
  {
    "id": "a0a4d3e2",
    "name": "Lontar Sunda",
    "statistics": "66 pages, 1, 526 train and 317 test word images 4, 555 train and 2, 816 test character images",
    "class": "61 character classes",
    "task": [
      "Binarization",
      "Text-line segmentation",
      "Character recognition",
      "Word transliteration",
      "Word spotting"
    ],
    "language": [
      "Sundanese"
    ],
    "document_type": "Sundanese manuscripts of the 15nth century from Garut, West Java and Indonesia",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "PNG",
      "TIFF"
    ],
    "reference": "8270066",
    "description": "\nThe Lontar Sunda dataset \\cite{8270066} is a collection of \\nth{15} century Sundanese palm leaf manuscripts from Garut, West Java, and Indonesia.\nThis dataset includes 66 pages with corresponding binarization, word-level, and character-level annotations.\nLontar Sunda was one of the datasets used in the ICFHR 2018 Competition on Document Image Analysis Tasks for Southeast Asian Palm Leaf Manuscripts \\cite{8583808}.\nThis competition hosted 4 challenges: A. Binarization, B. Text-line segmentation, C. Isolated character/glyph recognition, and D. Word transliteration.\nAs the original dataset paper did not include any benchmark results, the competition results are considered.\nFor Challenge A, systems were evaluated according to the \\ac{FM}, Peak SNR (PSNR), and Negative Rate Metric (NRM).\nThe best performing system on the Sundanese data used Gaussian operators and a non-linear function to enhance the images. \nThen, the enhanced images were finally segmented with a threshold of 0.9.\nIn Challenge B, the system evaluation was made using the \\ac{DR}, the \\ac{RA}, and the \\ac{FM}.\nThe system with the best values on the Sundanese collection, which was also the only submission for this task, used the binarized images from Challenge A and horizontal projection profile to perform line segmentation.\nThe character recognition challenge (C) was evaluated according to the recognition rate, and the highest value was obtained by a dense 100-layer \\ac{CNN} architecture that classified similar characters.\nFinally, in Challenge D, the best performing system achieved an 8.81\\% \\ac{CER} on the Sundanese set using a CNN-RNN encoder-decoder architecture with an attention mechanism."
  },
  {
    "id": "f09ab558",
    "name": "IMPACT",
    "statistics": "600000 images, 45000 ground-truthed images, 70000 word outlines",
    "class": "Text (+subcatecories) Graphics(+subcategories) Image, Line drawing Separator, Table Chart, Maths",
    "task": [
      "Layout analysis"
    ],
    "language": [
      "18 languages"
    ],
    "document_type": "European printed documents",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "TIFF",
      "JPEG",
      "JPEG2000"
    ],
    "reference": "10.1145/2501115.2501130",
    "description": "\nIMPACT \\cite{10.1145/2501115.2501130} is a large-scale dataset of over 600K images derived from different European libraries.\nThe provided PAGE XML files \\cite{5597587} contain the layout, reading order, and text transcription annotations for over 45K samples.\nThis collection also offers metadata information, including bibliographic information, digitization information, physical characteristics, copyright information, administrative information, and comments.\nThe dataset includes documents written in 18 different languages (Bulgarian, Catalan, Czech, Dutch, English, French, German, Greek, Hebrew, Latin, Norwegian, Old Church Slavonic, Polish, Portuguese, Russian, Slovenian, and Spanish) and 10 scripts (Bohori\u010dica, Cyrillic, French, Gaj, Greek, Hebrew, Latin, Latin/Gothic, Old Cyrillic, and Serif).\nA web interface provides access to the image samples and annotations, giving various options for the users to browse and search.\nThis paper did not present any benchmark results for this dataset."
  },
  {
    "id": "986fd9c7",
    "name": "DocExplore",
    "statistics": "1500 images 1464 queries",
    "class": "35 object categories (human faces decoration objects ornate initial letters etc.)",
    "task": [
      "Image retrieval",
      "Pattern localization"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Page images of manuscripts from the 10nth and 16nth from the Municipal Library of Rouen with graphics",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "72 dpi",
    "format": [
      "JPEG"
    ],
    "reference": "10.1117/1.JEI.26.1.011010",
    "description": "\nDocExplore \\cite{10.1117/1.JEI.26.1.011010} is a pattern spotting dataset that contains 1.5K images with more than 1.4K queries.\nThe images originate from 6 different manuscripts written between the \\nth{10} and \\nth{16} centuries.\nThe annotation process ends with 1,464 labeled objects belonging to 35 graphical object categories, where one sample constitutes the query image and the remaining objects from every category avail as retrieval outcomes.\nThe dataset was proposed for two tasks: image retrieval and pattern localization.\nAs baseline for the latter task a system that consists of an offline, online, and post-processing step initially presented in \\cite{EN2016149} is used.\nIn the offline step, the background is removed, a descriptor is used to find the object regions of interest, and finally a \\ac{VLAD} is created.\nThen, during the online step, a similarity distance calculation is performed between the extracted regions and the query image, then ranking was achieved through template matching.\nThe system achieved a 0.613 \\ac{mAP} for retrieval and a 0.111 for localization, while further results on each category were shown."
  },
  {
    "id": "a9c97d4b",
    "name": "ICDAR 2017 REID2017",
    "statistics": "5 train images 26 test images",
    "class": "Text Separator Graphic Image",
    "task": [
      "Layout analysis",
      "Text recognition"
    ],
    "language": [
      "Bengali",
      "English"
    ],
    "document_type": "Scanned images from printed books in Bengali from 1785-1909",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "TIFF"
    ],
    "reference": "8270161",
    "description": "\nThe REID 2017 Competition \\cite{8270161} held at ICDAR 2017 includes 26 evaluation images written in Bengali from 1785-1909 and an example set of 5 images for training.\nThe competition originally held two tasks, the Bengali text recognition and the Quarterly Lists challenge (tabular recognition in English and Bengali); however, there were no submissions for the latter challenge.\nThe organizers of the competition provided the image annotations in PAGE XML format \\cite{5597587} created using Aletheia \\cite{6065274}.\nThese annotations included layout region polygons, metadata such as heading, paragraph, captions, footer, etc., and reading order information.\n%The competition hosted two tasks, layout analysis and OCR, but focuses mostly on Bengali text recognition.\nThe Google Multilingual \\ac{OCR} that uses the Google Cloud vision API \\footnote{\\url{https://cloud.google.com/vision/}\\label{google}} achieved the highest flex \\ac{ca} compared to the other submissions; however, the \\ac{ca} of 75.4\\% suggests that there is plenty of room for improvement.\nThe same system, with a success rate of 78.4\\%, outperformed the other systems on the text region segmentation task."
  },
  {
    "id": "c2dfe56a",
    "name": "ICDAR19 RASM2019",
    "statistics": "20 train images 100 test images",
    "class": "Text Graphic Text line",
    "task": [
      "Page segmentation",
      "Text-line segmentation",
      "Text recognition"
    ],
    "language": [
      "Arabic"
    ],
    "document_type": "Arabic scientific manuscripts from 9nth-19nth CE",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "TIFF"
    ],
    "reference": "",
    "description": "\nThe next RASM competition after the one presented in section \\ref{sssec:rasm2018} is the ICDAR RASM19 \\footnote{\\url{https://www.primaresearch.org/RASM2019/}\\label{rasm2019}}, which focused on the recognition of archival Arabic scientific manuscripts.\nThis version of the competition offers 20 training images with PAGE XML annotations \\cite{5597587} and 100 test images to evaluate the systems.\nThe ground truth has the same format and content as the previous competition for the three tasks: text block detection, text-line detection/segmentation, and text recognition.\nAlthough the competition did not provide any detailed information due to the absence of a published paper, a graph was provided for every task containing the results of the submitted systems.\nA Google submission shows the highest success rate for the first task and an RDI system shows the highest success rate for the second task. \nFor text recognition in normalized text, a 77.58\\% flex \\ac{ca} is achieved again by the RDI system.\nWe suspect that the RDI winning systems are the same as those in the previous round of the competition, however it is not clear in the competition's website."
  },
  {
    "id": "050cd95e",
    "name": "HADARA80P",
    "statistics": "80 pages, 16720 words",
    "class": "Word segment classes",
    "task": [
      "Word-spotting"
    ],
    "language": [
      "Arabic"
    ],
    "document_type": "Handwritten Arabic documents of one writer about the taaum disease",
    "mode": [
      "Color"
    ],
    "resolution": "2882\u00d73650 pixels",
    "format": [
      "TIFF"
    ],
    "reference": "6980990",
    "description": "\nThe HADARA80P dataset \\cite{6980990} contains 80 handwritten Arabic pages originating from a single-author book about the taaum disease and its connections to religion.\nThe XML ground truth files provide the pages, text block, word coordinates, and transcription for every word.\nIn some cases, tag values accompany the words.\nThe total number of labeled words is 16,720.\nExperiments using a publicly available word spotting application\\footnote{\\url{http://www.corenum.com/products/ulysse/}\\label{ulysse}} are presented and an extension of the methods used in the application \\cite{Leydier2007TextSF, Leydier2009TowardsAO}, the HADARA word spotter, is proposed. \nThe original methods work by locating the zones of interest through gradients, while the proposed method employs curvature according to a threshold instead.\nThe resulting \\ac{mAP} based on the precision measures $p_{IR}$ and $\\overline{\\gamma_{LA}}$, presented in \\cite{6628824}, showed that the proposed system outperformed the already existing application on the HADARA80P and the George Washington datasets."
  },
  {
    "id": "b81999ad",
    "name": "POPP",
    "statistics": "Generic: 128 train, 16 val, 16 test pages 3, 840 train, 480 val, 480 test lines Belleville: 38 train, 5 val, 6 test pages 1, 140 train, 150 val, 180 test lines Chauss\u00e9e d'Antin: 625 train, 78 val, 77 test lines",
    "class": "Generic - Belleville - Chauss\u00e9e d'Antin 80 - 1 - 10 writers",
    "task": [
      "Text recognition",
      "Information extraction"
    ],
    "language": [
      "French"
    ],
    "document_type": "Pages from Paris census tables from 1926",
    "mode": [
      "Grayscale"
    ],
    "resolution": "200 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "10.1007/978-3-031-06555-2_10",
    "description": "\nThe POPP dataset \\cite{10.1007/978-3-031-06555-2_10} contains lines extracted from Paris census tables of 1926 and consists of three sub-datasets: the \"Generic dataset\", the \"Belleville\", and the \"Chauss\u00e9e d'Antin\".\nThe Generic dataset contains 80 double page images, one for every Paris district, each one from a different writer, and 4,800 lines divided in 3,840 train, 480 validation, and 480 test lines.\nThe Belleville dataset contains 49 pages and 1,470 lines from the Belleville district written from a single writer.\nThe Chauss\u00e9e d'Antin is a 10-writer set of 780 lines and 26 pages from the Chauss\u00e9e d'Antin census.\nPOPP includes grayscale images, their corresponding line bounding boxes in XML files, and the line labels in JSON format.\nThis work presents line recognition results of the \\ac{CER} and \\ac{WER} for each of the three datasets using an end-to-end hybrid attention network \\cite{Coquenet2022}.\nFinally, the paper presents a complete pipeline, with the steps of pre-processing, handwriting recognition, and domain knowledge integration, that extracted a vast number of information from the Paris census to be used as annotated dataset and improves the \\ac{CER} with the use of self-training.\n%%\n%%\n%%\n%%\n\\section{Observations \\& Discussion}\n\\label{sec:discussion}\nSeveral datasets exist for the tasks in the three categories that we presented in this paper: document classification, document structure, and content analysis.\nThere is a variation in languages, tasks, and sizes; however, no large-scale dataset seems to exist that can address various tasks and be used by the community for pretraining or transfer learning.\nVarious evaluation methods are also presented when benchmarking. \nNevertheless, it is difficult to directly compare datasets and techniques, as there is no universal evaluation that can directly compare the performance of systems on datasets.\nThe classification of objects on a page level is highly represented in document structure tasks. \nNevertheless, a document could also be considered as the whole manuscript collection.\nWe found six studies related to document classification, which means that this task is rarely addressed.\nOnly one dataset offers more than 35K pages, but the overall amount is relatively low.\nThe main focus of datasets is on Latin scripts, while others such as Arabic are also represented.\nSome scripts, such as Hebrew or Greek, are rarely represented.\nWe detected more meta-data information in several datasets that we categorized in the document structure and content analysis sections, as they are not used or included in the benchmarks.\nConsidering the document structure studies, we found only two datasets containing more than 10K images.\nAgain, there is a significant focus on Latin scripts; however, more languages are observed for this task as it is much more represented than document classification.\nA noticeable issue, in this case, is the comparison across databases as a variety of evaluation measures and benchmarks are used.\nWe propose harmonizing the evaluation metrics using the \\ac{mIoU} and \\ac{mAP} metrics (at 50\\%, 60\\%, 70\\%, and 80\\%).\nIn terms of annotation format, we note that most datasets use PAGE XML, three datasets use the COCO format, and only one dataset uses the VOC format.\nIt would be beneficial to establish a conversion between annotation formats to promote the use of state-of-the-art computer vision models for historical document analysis.\nThe content analysis task seems to have the most prominent representation in the set of datasets.\nIn this case, approximately 30\\% of the semantics-related studies include more than 1K images.\nThe majority of this percentage appears for isolated character recognition, which is reasonably the easiest case of samples one could obtain and manage in a database.\nIn general, there is an emphasis on \\ac{OCR}, but the level of detail differs (character, word, or line).\nRetrieval further focuses on text on the word, image, or writer level.\nLikewise, there is a focus on Latin scripts.\nStill, there is also a high representation of Asian scripts and the least representation on Arabic scripts.\nFinally, there is more interest in paleography, but we lack the representation of digits and tables as content.\n\\section{Conclusion}\n\\label{sec:conclusion}\nWe demonstrated a survey of historical document image datasets following a systematic literature review methodology.\nWe summarized 65 studies and clustered them considering the related general tasks that we defined.\nWe list the datasets in a table, connecting them to their corresponding section, and mark the possible tasks they include.\nFor every study, we tabulate detailed information about the statistics, tasks, document type, languages, input image visual aspects, annotations, and benchmark and quantitative performance analysis information.\nThis way, we facilitate researchers in finding the most fitting datasets and enable historical document image analysis.\nOur findings unveil a focus on Latin scripts and several evaluation methods, but not much compliance with deep learning trends.\nA clear size limitation on dataset samples is also obvious.\nAs future directions, we urge the need for large-scale datasets to apply state-of-the-art deep learning methods, the inclusion of more classification tasks using metadata information, and the harmonization of evaluation schemes for direct comparison across datasets.\n%"
  },
  {
    "id": "4d26862f",
    "name": "GloSAT",
    "statistics": "500 images",
    "class": "Heading, Header, Table body",
    "task": [
      "Table structure recognition"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Scanned printed and mixed pages from measurement logbooks from 1700 - modern days",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "10.1145/3476887.3476890",
    "description": "\nGloSAT \\cite{10.1145/3476887.3476890} is a table structure recognition dataset of 500 archival images, printed, handwritten or mixed, of meteorological records.\nThere are two types of ground truth in the dataset: individual cell and coarse segmentation cell annotations.\nIn addition to the conventional XML cTDaR19 format annotations, the dataset provides the widely used Pascal Visual Object Classes (VOC) format \\cite{Everingham2009ThePV} and extends these formats with cell information such as headers, page type, and table style.\nA benchmark evaluation of GloSAT (individual cell and coarse segmentation cell separately), cTDaR19, and their combination (+cTDaR19) using CascadeTabNet \\cite{Prasad2020CascadeTabNetAA} and CascadeTabNet with additional postprocessing proposed by the authors was presented.\nThis postprocessing step uses a 1-D DBSCAN clustering algorithm \\cite{10.1145/3068335} to infer vertical and horizontal lines of a table, assuming that only a subset of cells is needed to place the rest for a rectangular table.\nThe results on the \\ac{WA} F1 score showed that postprocessing helps the performance in all experimental cases."
  },
  {
    "id": "ba2b7993",
    "name": "BH2M",
    "statistics": "174 pages, 1740 licenses, 5498 lines, 56, 645 words",
    "class": "3, 360 word classes",
    "task": [
      "Layout analysis"
    ],
    "language": [
      "Old Catalan"
    ],
    "document_type": "Handwritten marriage record archives of Barcelona Cathedral",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi",
    "format": [
      "N/A"
    ],
    "reference": "6976764",
    "description": "\nThe Barcelona Historical Handwritten Marriages Database or BH2M \\cite{6976764} consists of 174 handwritten marriage record pages, where 100 pages are meant for training, 34 for validation, and 40 for testing.\nThe included pages were written in Old Catalan by a single writer between 1617 and 1619 and preserved in the Barcelona central archives.\nThe database provides the ground truth for layout analysis, text transcription, and semantic analysis.\nXML annotation files are organized hierarchically into text blocks, segmented lines, and text words for layout analysis.\nThe additional word transcriptions and semantics about the license, appearance order, date, and information about the wife and husband, may enable handwritten text recognition, word spotting, information extraction and understanding and context-aware algorithms.\nMoreover, baseline results of line segmentation \\cite{Mota2014AGA} using the \\ac{DR}, \\ac{RA}, and \\ac{FM} metrics, and segmentation-free \\cite{Vinciarelli2002OfflineCW} and -based \\cite{Almazn2012EfficientEW} word spotting algorithms using the \\ac{mAP} were presented."
  },
  {
    "id": "78dab2c4",
    "name": "ICDAR19 HDRC Chinese DB",
    "statistics": "12850 images",
    "class": "Text Non-text (background)",
    "task": [
      "Text recognition",
      "Layout analysis",
      "Text-line segmentation"
    ],
    "language": [
      "Chinese"
    ],
    "document_type": "Historical Chinese family records pages",
    "mode": [
      "Grayscale"
    ],
    "resolution": "Various",
    "format": [
      "PNG",
      "JPEG"
    ],
    "reference": "8977999",
    "description": "\nThis competition \\cite{8977999} presented a database of approximately 10K historical Chinese family record pages to evaluate systems for the tasks of (1) text recognition on extracted lines, (2) pixel-level layout analysis, and (3) text-line detection and recognition.\nMore specifically, the training set includes 11,715 pages derived from 37 different books along with their PAGE XML \\cite{5597587} and pixel-wise annotations, while the test set includes 1,135 images from 12 books.\nTo evaluate the submitted systems, Task 1 uses the edit distance (editDistance), Task 2 uses the \\ac{mIoU}, and Task 3 uses the total counted errors (totalErrors) of the output XML file.\nThe team that achieved the best results for all tasks used a Convolutional Recurrent Neural Network (CRNN) \\cite{Shi2017AnET} to recognize Chinese text, a Cascade R-CNN \\cite{Cai2018CascadeRD} to detect text lines, and a U-Net-shaped network for the pixel-wise classification.\nFor Task 2, the system that outperformed the others achieved a 99.96\\% IoU for the background class and 99.24\\% for the text class."
  },
  {
    "id": "14938bd8",
    "name": "ICDAR19 cTDaR19",
    "statistics": "Track A: 600 train, 199 test archival images Track B: 600 train, 150 test archival images",
    "class": "Archival Modern",
    "task": [
      "Table detection",
      "Table structure recognition"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Images with tables from archival accounting books record books timetables etc.",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "8978120",
    "description": "\nThe cTDaR competition of 2019 \\cite{8978120} held two tracks, Track A for table region detection and Track B for table recognition, and two datasets, modern and historical.\nThe historical document dataset includes civil records containing various handwritten tables sourced from 23 different institutions.\nFor the table detection track, the dataset provides 600 training and 199 test images.\nThe table recognition track provides 600 training and 150 test samples for two subtracks: B1, which provides the tables regions, and B2, which does not provide any a priori knowledge. \nHence, there is a need for both region and structure detection for B2. \nThe results from 11 teams for track A and two teams for track B were compared. \nThe winning team for track A achieved a \\ac{WA} F1 score of 0.94 for the historical documents by using a classifier to categorize modern and archive samples and Faster-RCNN \\cite{NIPS2015_14bfa6bb} for table detection. \nThen, they merged the overlapping regions that exceeded a given threshold value.\nFor the second track, the best submission achieved a \\ac{WA} F1 of 0.48 for B1 and 0.47 for B2. \nAn \\ac{FCN} was used to obtain the tables' guiding lines and junction points for broken line repair.\nThen, the cells were extracted through Connected Component Analysis and the row and column range were handled through a neighbor graph."
  },
  {
    "id": "c517feb3",
    "name": "IlluHisDoc",
    "statistics": "400 images",
    "class": "Illustration Text",
    "task": [
      "Baseline detection",
      "Illustration detection"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Document images from Gallica with various illustrations",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "monnier2020docExtractor",
    "description": "\nIn \\cite{monnier2020docExtractor}, a test set of Gallica images named IlluHisDoc was presented for segmentation generalizability purposes.\nThis set was split into four types of documents: (a) printed documents with drawings, photos, ornaments, and paintings; and manuscripts that contain (b) scientific graphs, (c) illuminations, and (d) drawings.\nMoreover, a segmentation method based on a ResNet-18 \\cite{He2016DeepRL} backbone encoder-decoder architecture was proposed.\nThe performance of this model was compared with the performance of Tesseract4\\footref{tesseract} and Mask-RCNN \\cite{he2017mask} using pre-training either on the synthetic dataset PubLayNet \\cite{zhong2019publaynet} or on SynDoc, a 10K image synthetic corpus created for this study.\nThe proposed method pretrained on SynDoc outperformed the other methods according to the \\ac{mIoU} results."
  },
  {
    "id": "7cb15da6",
    "name": "ABP NAF",
    "statistics": "ABP small: 180 pages ABP large: 1, 098 pages NAF: 488 pages",
    "class": "Text region Table region",
    "task": [
      "Table detection"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Death birth marriage and tax records",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "8978117",
    "description": "\nThe work presented in \\cite{8978117} introduces a method for layout and page sub-division that groups text-lines into semantic objects.\nIn order to evaluate the proposed method, they use the ABP dataset (ABP small) \\cite{8395184} and they further introduce and extension of it, ABP large, and the National Archive Finland (NAF) dataset.\nABP small, ABP large, and NAF contain 180, 1,098, and 488 pages, respectively, and were used for table rows, columns, and cells segmentation, where the F1 measure is reported, and shows the most promising results in the cell partition."
  },
  {
    "id": "da530f10",
    "name": "ICDAR 2017 Historical-WI",
    "statistics": "4783 images",
    "class": "394 train writers, 720 test writers",
    "task": [
      "Writer identification"
    ],
    "language": [
      "German",
      "French",
      "Arabic"
    ],
    "document_type": "Handwritten document pages from 13nth to 20nth century originating from Universit\u00e4tsbibliothek Basel",
    "mode": [
      "Color",
      "Binary"
    ],
    "resolution": "300 dpi",
    "format": [
      "JPEG",
      "PNG"
    ],
    "reference": "8270156",
    "description": "\nThe Historical-WI competition \\cite{8270156} focused on image retrieval based on writer identification.\nThe competition offered a set of 3,600 images of handwritten document pages ranging from the \\nth{13} to the \\nth{20} century for evaluation.\nThe test set originated from the Universit\u00e4tsbibliothek Basel and included 720 different writers.\nFor training, 1,182 images in color and binary format from 394 writers were provided and were different from the writers in the test set.\nThe submitted systems were evaluated using the \\ac{mAP} metric.\nThe system that achieved the highest \\ac{mAP} used feature vectors derived from binarized samples and the concatenation of their oriented Basic Image Feature (BIFs) columns histograms \\cite{7490135,NEWELL20142255}."
  },
  {
    "id": "82faadfa",
    "name": "VML-HD",
    "statistics": "680 pages 121, 636 sub-words 244, 553 characters",
    "class": "Book, Page number Sub-word id Location coordinates Arabic, Latin annotation Sub-word length",
    "task": [
      "Word-spotting",
      "Word recognition"
    ],
    "language": [
      "Arabic"
    ],
    "document_type": "Handwritten scripts from 5 books by different writers from 1088-1451",
    "mode": [
      "Color"
    ],
    "resolution": "6000\u00d7 4000 pixels",
    "format": [
      "TIFF"
    ],
    "reference": "8067751",
    "description": "\nVML-HD \\cite{8067751} is a database of Arabic handwritten documents that includes 680 pages from 5 different books of different writers.\nThis database can be used for handwriting recognition and word-spotting.\nThe annotations of the dataset include the book and page number, the segment id, bounding box coordinates for 121,636 sub-words and 244,553 characters, length of subword, and Arabic and Latin symbol transcriptions in Hadara XML format.\nWord spotting results using Radial Descriptor \\cite{6981050} and Radial Descriptor Graph \\cite{7814035} on a subset from every book and the 5 books combined were presented.\nThe Top1 - Top5 \\ac{DR} of the Radial Descriptor Graph method showed better performance on the combined set than the Radial Descriptor."
  },
  {
    "id": "801f39c4",
    "name": "HTR Benchmarks",
    "statistics": "ICFHR-2014: 433 pages, 11, 473 lines 106K running words, 550K characters ICDAR-2015: 796 pages, 21, 752 lines 186K running words, 955K characters ICFHR-2016: 450 pages, 10K lines 43K running words, 260K characters ICDAR-2017: 10K pages, 206K lines 1.7M running words, 8M characters",
    "class": "ICFHR-2014: 9K lexicon, 86 character set ICDAR-2015: 17K lexicon, 87 character set ICFHR-2016: 8K lexicon, 92 character set ICDAR-2017: 4K lexicon, 104 character set",
    "task": [
      "Text recognition"
    ],
    "language": [
      "English",
      "Early modern German"
    ],
    "document_type": "Handwritten document pages from the Bentham and the Ratsprotokolle (1470-1805) collections",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi or 75-300 dpi",
    "format": [
      "JPEG"
    ],
    "reference": "SANCHEZ2019122",
    "description": "\nThe work published in \\cite{SANCHEZ2019122} presents four benchmarks for historical document HTR and achieves state of the art results for four different competitions: ICFHR-2014 \\cite{6981116}, ICDAR-2015 \\cite{7333944}, ICFHR-2016 \\cite{Snchez2016ICFHR2016CO}, and ICDAR-2017 \\cite{8270157}. \nThe ICFHR-2014 dataset is a subset of the Bentham Papers \\cite{Causer2012BuildingAV} that contains 433 images with line detection and recognition ground truth in PAGE XML format.\nSimilarly, the ICDAR-15 competition  contains Bentham page images, but presenting a more difficult layout than those of ICFHR-2014.\nThis dataset consists of different subsets that include line images with their corresponding line transcriptions aligned, or images with page-level transcriptions, but no alignment.\nThe ICFHR-2016 dataset includes 450 single-block page images derived from the German Ratsprotokolle collection, that contain approximately 10K lines and 43K running words.\nThe provided ground truth is at line-level.\nThe three mentioned competitions include a Restricted and an Unrestricted Track.\nFinally, the ICDAR-2017 competition provides 10,172 images distributed across two training and two test subsets.\nThe data provided come from the Alfred Escher Letter (AEC) and other German collections and present heterogeneous writing styles.\nThe competition includes a Traditional challenge for simple transcription and an Advanced challenge for transcription, but with a pre-step of line detection.\nFor the benchmarking, a CRNN with four convolutional and three recurrent layers is used for character optical modeling and enhanced with the use of N-gram language models on the output character probabilities.\nWith this enhancement, the work achieves the lowest \\ac{CER} and \\ac{WER} for all cases."
  },
  {
    "id": "04c8b0d2",
    "name": "ICDAR 2019 REID 2019",
    "statistics": "81 images",
    "class": "Text Separator Graphic Image",
    "task": [
      "Layout analysis",
      "Text recognition"
    ],
    "language": [
      "Bengali",
      "English"
    ],
    "document_type": "Scanned images from printed books in Bengali from 1713-1914",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "TIFF"
    ],
    "reference": "8978191",
    "description": "\nThe REID2019 competition \\cite{8978191} is an extension of the previously mentioned REID2017 competition (Section \\ref{sssec:reid2017}).\nThis competition provided 25 labeled images with the same annotation format and content as the previous competition and a balanced test set of 56 images written in English and Bengali.\n%The organizers of the competition provide the image annotations in PAGE XML format cretaed using Aletheia.\n%These annotations include layout region polygons, metadata such as heading, paragraph, captions, footer, etc, and reading order information.\nThis competition hosted two tasks, layout analysis and text recognition, but focused mostly on Bengali text recognition.\nAgain, the Google Multilingual \\ac{OCR} achieved the highest flex \\ac{ca} in the text recognition task and success rate in the text region page segmentation.\nThe results from this competition were slightly better than those in 2017 but still remain quite low, and the authors suggested a focus on preprocessing for better performance."
  },
  {
    "id": "e1057f9b",
    "name": "Tripitaka Koreana in Han (TKH)",
    "statistics": "1000 images, 23471 lines, 323491 characters",
    "class": "1, 471 character classes",
    "task": [
      "Character segmentation",
      "Character recognition"
    ],
    "language": [
      "Chinese"
    ],
    "document_type": "Chinese historical documents",
    "mode": [
      "N/A"
    ],
    "resolution": "N/A",
    "format": [
      "N/A"
    ],
    "reference": "Yang2018DenseAT",
    "description": "\nThe work presented in \\cite{Yang2018DenseAT} introduces two datasets for Chinese character detection and recognition, the Tripitaka Koreana in Han (TKH) and the Multiple Tripitaka in Han (MTH), created using the publicly available TKH images from the Tripitaka Koreana Institute.\n%The images were annotated by detecting the text lines and then segmenting them using beam-search to obtain the bounding box annotations.\nFor every character bounding box that the dataset includes, a character label was further provided.\nThe TKH consists of 1K pages, 23,471 lines, 323,491 characters, and 1,471 character classes, while the MTH contains 500 images, 17,178 lines, 197,886 characters, and 3,664 character classes.\nThe two datasets differ in terms of challenge, as the MTH dataset has a more complex character size uniformity, making the creation of bounding box annotations even harder.\nA three-part pipeline called Recognition Guided Detector (RGD) was proposed. \nFirst, segmentation is performed for every line. \nA Recognition Guided Proposal Network (RGPN) generates context information, and finally, a detector uses that information to find the characters in every line.\nFinally, several experiments were performed on the two datasets using the proposed system with and without a VGG-16 \\cite{Simonyan2015VeryDC} backbone and its performance was compared with other well-known object detection frameworks using either the whole image or text lines as input.\nThis method seems to perform comparably to other methods using fewer parameters."
  },
  {
    "id": "6347b08b",
    "name": "KERTAS",
    "statistics": "2502 images, 135 books",
    "class": "14 islamic centuries",
    "task": [
      "Age detection",
      "Writer identification"
    ],
    "language": [
      "Arabic"
    ],
    "document_type": "Arabic manuscripts spanning 14 islamic centuries",
    "mode": [
      "Color"
    ],
    "resolution": "High-resolution",
    "format": [
      "N/A"
    ],
    "reference": "adam2018kertas",
    "description": "\nThe KERTAS dataset \\cite{adam2018kertas} consists of handwritten Arabic manuscripts from the Qatar National Library intended for age and writer detection.\nThis dataset contains 2,502 high-resolution document images and their corresponding date annotation according to the Islamic century in which they were written.\nThe dataset provides the additional source, manuscript name, description, writer name, and ID information in XML format for every manuscript.\nFurthermore, an age detection algorithm based on sparse representations was introduced in this work and results on the dataset using different image size inputs were shown. \nThis method was also compared with three different writing style feature algorithms, Run Length \\cite{4107573}, Edge Direction \\cite{BRINK2012162}, and Edge Hinge \\cite{10.1016/j.patrec.2013.03.020}, with 3-NN as the classifier.\nBoth sets of experiments were made using predefined and random splits, and the predefined splits performed better in all cases.\nThe proposed method on the $50 \\times 50$ image size achieved the best performance."
  },
  {
    "id": "dac94fb4",
    "name": "ESPOSALLES LICENCES",
    "statistics": "173 pages, 1747 acts, 5447 lines, 60777 words, 328229 characters",
    "class": "3, 465 lexicon size 85 character classes, Handwritten",
    "task": [
      "Text recognition"
    ],
    "language": [
      "Spanish"
    ],
    "document_type": "Handwritten documents from marriage license books",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "ROMERO20131658",
    "description": "\nThe ESPOSALLES database \\cite{ROMERO20131658} is a collection of ancient marriage license documents separated into the LICENSES and the INDEX subsets.\nA single-writer book written in old Catalan is the main content of the LICENSES set, and is comprised of 173 pages and 1,747 licenses.\nFor every page, the subset includes the main text block bounding box, the text line within the text block coordinates, the license label, and the transcription for every line, word, and character in the main block.\nThe INDEX subset, which includes 29 pages of the initial indexes of two volumes by a single writer created between 1491 and 1495.\nSimilar to the LICENSES part, INDEX contains text and line layout as well as transcription annotations.\nBoth subsets provide dataset splits for cross-validation.\nFinally, baseline results using Hidden Markov Models (HMM) \\cite{Toselli2004IntegratedHR} and a BLSTM \\cite{Graves2009855} with two feature sets, PRHLT \\cite{Toselli2004IntegratedHR} and IAM \\cite{10.5555/505741.505745}, showed the efficiency of neural networks with larger datasets for handwriting recognition.\nThe database was further used in the ICDAR17 Competition on Information Extraction in Historical Handwritten Records \\cite{8270158}.\nThe aim there was to detect and assign name entities to semantic categories (name, surname, occupation, etc.) for two Tracks: Basic and Complete, which also contains the person (husband, wife, etc.).\nThe team that obtained the highest average score with word-level segmentation used a ResNet-based unigram system for character recognition and named entity recognition, while with line-level segmentation, the best method used a RNN-LSTM with \\ac{CTC}."
  },
  {
    "id": "54aa8504",
    "name": "HisClima",
    "statistics": "208 pages, 33, 739 lines 66, 814 running words 15, 471 relevant information",
    "class": "1, 483 lexicon, 76 characters",
    "task": [
      "Text recognition",
      "Layout analysis",
      "Information extraction"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Pages from a ship weather log book that sailed from 1880-1881",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "N/A"
    ],
    "reference": "9412210",
    "description": "\nHisClima \\cite{9412210} is a database of handwritten weather ship log book pages from 1880 to 1881 that contains both layout annotations of blocks, columns, rows, and lines and transcription annotations with relevant information such as number of cells in the tables.\nThe dataset comprises 208 pages with tables and 211 pages with descriptive text.\nBaseline experiments are performed for text recognition, line segmentation, and information extraction.\nA CRNN  with CTC loss performing on line images was used for the recognition task with and without a language model (LM) and evaluated according to the \\ac{WER} and \\ac{CER}.\nThe neural network architecture presented in \\cite{Quirs2018MultiTaskHD}, that performs geometric and logical layout analysis, was used for the task of line segmentation.\nFinally, for the information extraction for cell position and line geometry an information retrieval on tables method without segmentation based in \\cite{8563224} was used.\nThe two latter tasks were evaluated according to precision, recall, and F1 scores."
  },
  {
    "id": "54d8709c",
    "name": "Newspaper Navigator",
    "statistics": "3559 pages, 48409 annotations",
    "class": "Photograph, Illustration, Map, Comic/Cartoon Editorial cartoon, Headline, Advertisement, Printed",
    "task": [
      "Visual content recognition",
      "Text recognition"
    ],
    "language": [
      "English"
    ],
    "document_type": "Historical newspaper pages from the Chronicling America corpus",
    "mode": [
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "10.1145/3340531.3412767",
    "description": "\nThe Newspaper Navigator \\cite{10.1145/3340531.3412767} is a dataset extracted from the Chronicling America historical newspaper collection.\nThis dataset was created by employing an object detection pipeline over the 16.3 million collected pages that extracts visual and headline content. \nThe dataset provides 3,559 images with 48,409 COCO format annotations \\cite{Lin2014MicrosoftCC} for easy detection across 7 classes: headline, photograph, illustration, comic, map, editorial cartoon, and advertisement.\nThe textual content of the predicted bounding boxes is further rendered for \\ac{OCR} purposes and ResNet-18 and ResNet-50 embeddings \\cite{He2016DeepRL} for the different visual category crops. \nAdditional metadata in CSV format contain information such as file path, image URL, page URL, publication date, page sequence number, edition sequence number, batch name, LCCN, bounding box coordinates, prediction score, \\ac{OCR}, place of publication, geographic coverage, newspaper name, and newspaper publisher.\nA fine-tuned Faster-RCNN model \\cite{NIPS2015_14bfa6bb} with an R50-FPN backbone achieved a \\ac{mAP} of 63.4\\% on the validation set.\nThese results also included the AP for every class.\nThe authors further chose 500 pages randomly from 1850-1875 and 1875-1900, treating them as test sets, and presented the \\ac{mAP} on the most frequently appearing classes: the headline, the advertisement, the illustration, and the one class (all visual content into 1 class).\nThese results were slightly worse than the results on the validation set, especially in the case of the 1850-1875 test set."
  },
  {
    "id": "7f2f4406",
    "name": "BIR Database",
    "statistics": "35 documents, 285 pages, 2106 bold words, 5745 italic words, 80168 regular words, 880169 total words",
    "class": "Bold, Italic, Regular",
    "task": [
      "Font-style classification",
      "Word detection"
    ],
    "language": [
      "French",
      "Latin",
      "Other"
    ],
    "document_type": "Printed pages from sale catalogues and exhibitions from the 19nth and 20nth centuries",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "10.1145/3476887.3476913",
    "description": "\nThe Bold-Italic-Regular (BIR) database \\cite{10.1145/3476887.3476913} consists of printed historical document pages with word bounding boxes and three font classes (bold, italic, and regular) meant for word detection and font style classification.\nThe BIR database includes 285 scanned pages from various catalogues from the \\nth{19} and \\nth{20} centuries written in French, Latin, or other languages.\nBaseline results using 50\\% training, 25\\% validation, and 25\\% test random splits (TVT), and a 5-fold cross-validation (CV5) were presented.\nFor word detection, YOLOv5m was utilized \\cite{glenn_jocher_2021_4418161}, and for style classification, MobileNetV2 \\cite{sandler2019mobilenetv2} and Xception \\cite{chollet2017xception} were utilized.\nAll models were evaluated according to the F1 score.\nThe MobileNetV2 style classification results were also compared with the results of a human expert on 1K randomly chosen words.\nThe results showed a similar performance between the model and the human expert.\n%********************************************************************************"
  },
  {
    "id": "81208653",
    "name": "OBC306",
    "statistics": "4, 024 distinct characters 309, 551 samples",
    "class": "306 character categories",
    "task": [
      "Character recognition"
    ],
    "language": [
      "Oracle"
    ],
    "document_type": "Patch samples derived from oracle-bone publications",
    "mode": [
      "Grayscale"
    ],
    "resolution": "1:2 ratio Height: 50-150 pixels Width: 0-100 pixels",
    "format": [
      "N/A"
    ],
    "reference": "8978032",
    "description": "\nOBC306 \\cite{8978032} is a dataset of 309,551 images for Oracle Bone character recognition distributed across 306 character classes.\nThis dataset consists of patch samples derived from different sources from full image publications of oracle bones.\nFor patch extraction, an oracle bone character list and dictionary were used to retrieve all characters and extract them from the source images to assign them to a class and a specific encoding.\nThe challenges faced in the dataset are the class imbalance and the numerous variants of each character.\nThe evaluation results of widely used \\ac{CNN} architectures \\cite{He2016DeepRL, Simonyan2015VeryDC, NIPS2012_c399862d, 7298594}, and a classical method of HOG descriptors with SVM \\cite{1467360} were presented, and Inception-v4 \\cite{7298594} achieved the best performance.\nAlthough the dataset is hand carved, we characterize it as handwritten in Table \\ref{tab:structure_content_class} for homogeneity reasons. "
  },
  {
    "id": "5fb6af2e",
    "name": "HJDataset",
    "statistics": "2, 271 Images 25K Elements",
    "class": "Page frame Row Title, Text region Title, Subtitle Other",
    "task": [
      "Layout analysis",
      "Element extraction"
    ],
    "language": [
      "Japanese"
    ],
    "document_type": "Biography scans",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "Shen_2020_CVPR_Workshops",
    "description": "\nHJDataset \\cite{Shen_2020_CVPR_Workshops} was introduced in the Text and Documents in the Deep Learning Era Workshop hosted by CVPR 2020. \n%The authors followed a similar format as the modern document dataset PubLayNet \\cite{zhong2019publaynet} but for Japanese historical documents. \nThe HJDataset contains 2,271 pages from Japanese biography scans for layout analysis with COCO annotations \\cite{Lin2014MicrosoftCC}, derived by a semirule-based method. \nFurthermore, the ground truth includes reading order and dependency structure information. \nBenchmark results of experiments with popular object detection models such as Faster-RCNN \\cite{NIPS2015_14bfa6bb}, Mask-RCNN \\cite{he2017mask}, and Retinanet \\cite{lin2017focal} provided by Detectron2 were shown \\cite{wu2019detectron2}. Moreover, few-shot and zero-shot learning results using COCO weights were presented."
  },
  {
    "id": "31673181",
    "name": "PHTD",
    "statistics": "140 documents, 1787 handwritten lines, 27.073 words",
    "class": "3 Types of text, Handwritten",
    "task": [
      "Text-line segmentation",
      "Sentence recognition/understanding",
      "Word segmentation",
      "Word recognition",
      "Character segmentation",
      "Word spotting",
      "Writer identification"
    ],
    "language": [
      "Persian"
    ],
    "document_type": "Persian handwritten text documents",
    "mode": [
      "Grayscale"
    ],
    "resolution": "300 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "6121553",
    "description": "\nPHTD \\cite{6121553} is a 140-page dataset of handwritten documents in the Persian language.\nThe dataset includes 1,787 text-lines and 27,073 words for text recognition and word and line segmentation tasks.\nFor the former task, a unicode text file is included for every page, and for the latter, a pixel-labeled file is provided as the ground truth.\nTwo algorithms were utilized for the task of text-line segmentation.\nThe Potential Piece-wise Separation Line (PPSL) method \\cite{10.1007/s10044-011-0226-x} obtained an 89.43\\% segmentation accuracy, while the other method proposed by Alaei et al. \\cite{Alaei2011ANS}, outperformed PPSL with an accuracy of 94\\%.\nIn both methods, each document was split vertically into stripes. "
  },
  {
    "id": "7523d7b6",
    "name": "CASIA-AHCDB",
    "statistics": "11937 images, 2200000 characters",
    "class": "10350 character categories",
    "task": [
      "Character recognition"
    ],
    "language": [
      "Chinese"
    ],
    "document_type": "Chinese ancient handwritten document pages from the Complete Library in Four Sections (style-1) and the Ancient Buddhist Scriptures (style-2)",
    "mode": [
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "GNTX"
    ],
    "reference": "8978010",
    "description": "\nCASIA-AHCDB \\cite{8978010} is a database of 11,937 handwritten Chinese document pages.\nFor the task of character recognition, the database provides 2.2M handwritten characters belonging to 10,350 different classes.\nThe database distributes these elements across two different datasets: (a) the Complete Library in Four Sections (AHCDB - style1) and (b) the Ancient Buddhist Scriptures (AHCDB - style2).\nThen, each dataset is split into a Basic Category Set (BC) for basic character recognition, an Enhanced Category Set (EC) for open-set character recognition, and a Reserved Category Set (RC) for other recognition purposes.\nTo benchmark the database, a \\ac{CNN} \\cite{Zhang2017OnlineAO} and a Convolutional Prototype Network (CPN) \\cite{Yang2018RobustCW} were used and experiments were performed with only the \\textit{Basic Category Set} and with the combination of the \\textit{Basic} and \\textit{Enhanced Category Sets} for every dataset. \nMoreover, the transfer of information from the style1 to the style2 dataset with direct train-test and fine-tuning was attempted, which performed best among the two methods."
  },
  {
    "id": "c58e5d8d",
    "name": "Kuzushiji",
    "statistics": "K-MNIST: 60K train - 10K test images K-49: 232, 365 train - 38, 547 test images K-Kanji: 140, 426 images",
    "class": "K-MNIST: 10 character classes K-49: 49 character classes K-Kanji: 3, 832 character classes",
    "task": [
      "Character recognition"
    ],
    "language": [
      "Kuzushiji  (cursive Japanese)"
    ],
    "document_type": "Character images from scanned documents",
    "mode": [
      "Grayscale"
    ],
    "resolution": "28\u00d728 64\u00d764 pixel resolution",
    "format": [
      "PNG"
    ],
    "reference": "Clanuwat2018DeepLF",
    "description": "\nThe full Kuzushiji dataset \\cite{Clanuwat2018DeepLF} consists of three parts: the Kuzushiji-MNIST, the Kuzushiji-49, and the Kuzushiji-Kanji.\nThe whole dataset is comprised of printed books from the \\nth{18} century written in cursive Japanese or Kuzushiji.\nThe K-MNIST subset includes 70K 28$\\times$28 grayscale images of 10 Kuzushiji character classes to resemble the MNIST and Fashion-MNIST datasets but is even more challenging.\nKuzushiji-49 contains 270,912 images of the same pixel resolution and mode as K-MNIST, including 49 character classes.\nFinally, Kuzushiji-Kanji is a subset of 140,426 64$\\times$64 grayscale images of 3,832 Kanji characters.\nThe two latter subsets are considered quite imbalanced. \nBenchmark results on the K-MNIST and Kuzushiji-49 were presented using a 4-nearest neighbor classifier, a 2-layer CNN, ResNet-18 \\cite{10.1007/978-3-319-46493-0_38}, ResNet-18 with input mixup \\cite{Zhang2018mixupBE}, and  ResNet-18 with manifold mixup regularizer \\cite{verma2018manifold}. \nThe performance of these models were compared using MNIST. All models had the highest test accuracy on the MNIST test set, followed by K-MNIST, and finally Kuzushiji-49. \nThe best performing model for the K-MNIST and Kuzushiji-49 test sets was ResNet-18 with manifold mixup, while for MNIST, it was the simple ResNet-18 model.\nDomain transfer was further explored from from Kuzushiji-Kanji to modern Kanji (stroke format).\nTwo Variational Autoencoders \\cite{Kingma2014AutoEncodingVB, JimenezRezende2014StochasticBA} were used to create the old (Kuzushiji) and new (Modern) latent space embeddings.\nThen, a Mixture Density Network \\cite{Bishop94mixturedensity} predicted the probability of the new embedding given the old embedding. \nFinally, a Sketch-RNN \\cite{Ha2018ANR} conditioned on the new latent space created modern Kanji stroke image versions of Kuzushiji."
  },
  {
    "id": "e9f46e58",
    "name": "ICFHR18 Asian Palm Leaf",
    "statistics": "Bin/tion: 50 train-50 test Balinese, 23 train-23 test Khmer, 31 train-30 test Sundanese images Text line: 47 train-49 test Balinese, 50 train-200 test Khmer, 31 train-30 test Sundanese images OCR: 11K train-7K test Balinese, 113K train-90K test Khmer, 4.5K train-2.8K test Sundanese images Translit.: 15K train-10K test Balinese, 16K-7.7K test Khmer, 1.4K train-318 test Sundanese images",
    "class": "133 Balinese, 111 Khmer and 60 Sundanese character classes",
    "task": [
      "Binarization",
      "Text-line segmentation",
      "Character recognition",
      "Transliteration"
    ],
    "language": [
      "Balinese",
      "Khmer",
      "Sundanese",
      "Latin"
    ],
    "document_type": "Manuscripts",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "PNG",
      "TIFF"
    ],
    "reference": "8583808",
    "description": "\nThe Southeast Asian palm leaf manuscripts competition \\cite{8583808} offered four tasks: binarization, text-line segmentation, isolated character/glyph recognition, and word transliteration. \nThis competition included manuscripts in three languages: Balinese, Khmer, and Sundanese.\nFor Balinese, Khmer, and Sundanese language sets, the Amadi\\_Lontarset dataset presented in Section \\ref{sssec:amadi_lontarset}, the SleukRith dataset presented in Section \\ref{sssec:sleukrith}, and the Lontar Sunda dataset presented in Section \\ref{sssec:SUNDANESE_PALM_LEAF}, respectively, were utilized. \n The results for each separate language set are presented in the corresponding dataset sections.\nIn this section, we will present the overall results of the competition.\nThe system that obtained the highest FM and PSNR values for binarization used Gaussian operators and a nonlinear function to enhance the images. \nThen, the enhanced images were finally segmented with a threshold of 0.9.\nThe same system achieved a value of 0.17 NRM that underperformed the best value by only 0.01.\nIn challenge B, text-line segmentation, the best and only \\ac{DR}, \\ac{RA}, and \\ac{FM} values were achieved by using the binarized images from Challenge A and the horizontal projection profile to perform line segmentation.\nChallenge C, character recognition, was evaluated according to the recognition rate, and the highest value was obtained by using a dense 100-layer \\ac{CNN} architecture that classifies similar characters.\nFinally, in Challenge D, the best performing system achieved a 5.62\\% \\ac{CER} on the mixed sets using a CNN-RNN encoder-decoder architecture with an attention mechanism."
  },
  {
    "id": "d4eedb9d",
    "name": "HBA",
    "statistics": "4000 images",
    "class": "Graphics Normal text Capitalized text Handwritten text Italic text Footnote text",
    "task": [
      "Layout analysis"
    ],
    "language": [
      "Latin          Italian"
    ],
    "document_type": "Manuscripts Printed pages",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "300 dpi 400 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "mehri:hal-01637826",
    "description": "\nHBA 1.0 \\cite{mehri:hal-01637826} is a collection of 11 books with 4,436 pages  of manuscripts and printed documents from the Gallica digital library from the \\nth{13} to the \\nth{19} century written in different scripts and languages. \nThis dataset provides either ground truth images, where every foreground pixel has a different color according to the class it belongs to out of the six predefined classes, which include graphics, main text body, capitalized text, handwritten text, italic text, footnote text, or text files containing the label of every pixel. \nAs a baseline, this paper presented the pixel-level classification accuracy (CA) of a texture-based layout segmentation method \\cite{Mehri2015ATP} that, averaging over 4 books, was 75.9\\%.\nThe ICDAR 2017 and ICDAR 2019 Competition on Historical Book Analysis \\cite{8978192} introduced this dataset into two tasks: textual and graphical content discrimination at pixel-level and pixel-level annotation of textual content. \nThe highest overall performance for both tasks was achieved by an \\ac{FCN} that performed on $512\\times512$ patches using a weighted cross entropy loss function.\n%"
  },
  {
    "id": "a0b6c29f",
    "name": "READ-BAD",
    "statistics": "2036 pages 132124 baselines",
    "class": "Layout classes for text regions e.g. paragraphs",
    "task": [
      "Baseline detection"
    ],
    "language": [
      "Latin"
    ],
    "document_type": "European archival documents from 1470-1930",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "8395221",
    "description": "\nThe READ-BAD dataset \\cite{8395221} contains 2,035 images of simple and complex documents with 132,123 baselines for baseline detection. \nThe images were derived from 9 European archives written from 1470 to 1930, and as ground truth, the PAGE XML annotation format \\cite{5597587} was used. \nThe ICDAR 2017 Competition on Baseline Detection (cBAD) \\cite{8270153} used the READ-BAD dataset for Track A, text-line segmentation on simple documents, and Track B, text-line location on complex documents with noise and various layout elements, providing only the page.\nThis dataset provides 216 training images for the simple layouts and 270 for the complex layouts.\nAn evaluation scheme for baseline detection was introduced using the R-value, P-value, and F-value. \nThe R-value and the P-value have similarities to the recall and precision metrics, respectively, while the F-value is the harmonic mean of the two values. \nThe best performing system for both tracks used a U-Net-based architecture (DMRZ submission) that extracted baselines and text regions of interest. \nMoreover, a layout classification was performed according to the provided classes as a preprocessing step. \nPostprocessing further improved the predicted baselines through detection error pruning and baseline fragment merging."
  },
  {
    "id": "28ad733c",
    "name": "GRPOLY-DB",
    "statistics": "399 pages, 15084 lines, 102596 words, 171511 characters",
    "class": "More than 270 character classes",
    "task": [
      "Word segmentation",
      "Text-line segmentation",
      "Text recognition",
      "Character recognition",
      "Word spotting"
    ],
    "language": [
      "Greek"
    ],
    "document_type": "Handwritten and printed document pages from 1838-1977 that contain greek polytonic characters",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "N/A"
    ],
    "reference": "7333841",
    "description": "\nThe Greek polytonic database (GRPOLY-DB) \\cite{7333841} contains images of printed and handwritten documents from different sources distributed across four subsets: GRPOLY-DB-Handwritten, GRPOLY-DB-MachinePrinted-A, GRPOLY-DB-MachinePrinted-B, and GRPOLY-DB-MachinePrinted-C. \nThe documents were written or printed in the old polytonic system from 1838 to 1977.\nThe overall dataset includes 399 pages, 15,084 text lines, 102,596 words, and 171,511 characters with ground truth.\nThe provided annotations include text and word-level segmentation information and transcriptions for text recognition and isolated character recognition.\nLayout-related and content-related experimental results were shown on the dataset using various methods. \nFor text-line segmentation, a shredding-based system \\cite{5277573}, which achieved a value of 94.58\\% for the average F-measure on the four subsets, outperformed a Hough transform method. \nFor word segmentation, sequential clustering \\cite{953781} and Gaussian mixture-based methods \\cite{10.1016/j.patcog.2008.12.016} were applied. \nThe former method achieved the highest average FM of 94.85\\%.\nThe GRPOLY-DB-MachinePrinted-B was the only subset used for isolated character recognition in two scenarios, one scenario with all character instances and another with 30 random samples per class.\nThus, there were 143,051 and 3,750 characters per scenario, respectively.\nTwo algorithms were evaluated for both scenarios, HoG features \\cite{1467360} with an SVM classifier and adaptive window features \\cite{6065492} with a k-NN classifier.\nFor both systems, the first scenario obtained the highest \\ac{RA}. \nBetween the two methods, HoG features with the SVM classifier obtained the highest metric values in both scenarios.\nIn addition, \\ac{OCR} experiments were performed at the character and word levels using Tesseract\\footref{tesseract} and ABBY FineReader, with the latter performing the best. Finally, the \\ac{mAP} was presented for query-by-example word spotting, where profile features with dynamic time warping (DTW) \\cite{Rath2006WordSF} for feature vector comparison obtained the best results on the whole dataset."
  },
  {
    "id": "3e2d643a",
    "name": "CFRAMUZ",
    "statistics": "7 novels 64 pages 18, 027 words",
    "class": "2, 998 unique words",
    "task": [
      "Word spotting"
    ],
    "language": [
      "French"
    ],
    "document_type": "Charles Ferdinand Ramuz's novels written from 1910 - 1946",
    "mode": [
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "TIFF"
    ],
    "reference": "Arvanitopoulos2017AHF",
    "description": "\nThe CFRAMUZ dataset \\cite{Arvanitopoulos2017AHF} includes grayscale image pages from handwritten novels by Charles Ferdinand Ramuz in French between 1910 and 1946.\nText and XML annotation files contain the unique word ID, coordinates, width and height of word bounding boxes, word line number, word number in the current line, and word transcription for word spotting without segmentation purposes.\nThe following methods were evaluated according to Precision-Recall curves: Word Spotting and Recognition with Embedded Attributes (EAWS) \\cite{6857995}, Efficient Exemplar Word Spotting (EEWS) \\cite{Almazn2012EfficientEW}, Bag-of-Visual-Words Word Spotting (BoVWWS) \\cite{Rusiol2011BrowsingHD}, and Fisher Kernels Word Spotting (FKWS) \\cite{5277774}.\nThe \\ac{mAP} of these algorithms were compared on the introduced datasets with the performance on the George Washington (GW) and the Lord Byron (LB) datasets.\nAlthough this is a single-writer dataset, some variation in terms of writing style occured due to the year range.\nTherefore, additional experiments using splits according to style were conducted."
  },
  {
    "id": "5de344a6",
    "name": "RODRIGO",
    "statistics": "853 pages, 20357 lines, 232000 words",
    "class": "17K word lexicon 115 character set, Handwritten",
    "task": [
      "Text recognition",
      "Text block detection",
      "Baseline detection"
    ],
    "language": [
      "Old Castilian"
    ],
    "document_type": "Page images of a manuscript from 1545 written in old Castilian by one writer",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi",
    "format": [
      "N/A"
    ],
    "reference": "serrano-etal-2010-rodrigo",
    "description": "\nThe RODRIGO database \\cite{serrano-etal-2010-rodrigo} contains data derived from a manuscript written in 1545 in old Castilian by one writer.\nThe database follows a similar strategy as the GERMANA database in creation, ground truth, and experimental baseline.\nIt includes 853-page images of one column text blocks, where each block is annotated with a bounding rectangle, and then each line within it with the corresponding baseline.\nThe annotations further include transcriptions for every line, which results in a total of 20,357 text lines and 231K words as ground truth.\nBaseline results were provided for the task of handwriting recognition using the same model and processes as in Section \\ref{sssec:germana}, by using 20 blocks of 1K lines, and a \\ac{WER} of 36.5\\% was achieved on the last block. "
  },
  {
    "id": "75f857a1",
    "name": "GERMANA",
    "statistics": "764 pages, 20529 lines, 217200 words",
    "class": "27.1K 50 word lexicon 115 character set, Handwritten",
    "task": [
      "Text recognition",
      "Text block detection",
      "Baseline detection"
    ],
    "language": [
      "Spanish",
      "Catalan",
      "Latin",
      "French",
      "German",
      "Italian"
    ],
    "document_type": "Page scans of Spanish manuscript from 1891 on the life of Germana de Foix",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi",
    "format": [
      "N/A"
    ],
    "reference": "5277691",
    "description": "\nGERMANA \\cite{5277691} is a database of 764 scanned pages from an 1,891 manuscript written in Spanish.\nThe pages contain 21K text lines and 217K words.\nCatalan, French, Latin, German, and Italian may also appear in some parts of the text.\nThe ground truth comprises bounding box annotations for the text blocks, straight baselines for every text line, and line-by-line transcriptions.\nAlthough the database annotations contain both layout and text information, the baseline experiments were limited to the task of handwriting recognition. \nThe transcription \\ac{WER} per block for handwriting recognition experiments were presented using a system that combines Hidden Markov Models for text recognition and n-grams for language modeling \\cite{4377054}.\nThe results contained the first 180 pages of the database separated into blocks of 20 pages and were presented by adding each block consecutively.\nA 37\\% \\ac{WER} was achieved for the last two blocks, while the error is higher in the first blocks, where more out-of-vocabulary words were presented."
  },
  {
    "id": "c45e0a4b",
    "name": "ICDAR 2017 CLAMM",
    "statistics": "ICFHR16 CLaMM train images Task 1, 3: 2K test images Task 2, 4: 1K test images",
    "class": "Same script classes as ICFHR16 CLaMM, 15 date classes starting from before 1000 C.E. to 1600 C.E.",
    "task": [
      "Script type classification",
      "Date prediction"
    ],
    "language": [
      "Latin"
    ],
    "document_type": "Manuscripts from the French catalogues the BVMM and Gallica",
    "mode": [
      "Grayscale",
      "Color"
    ],
    "resolution": "Task 1, 3: 300 dpi Task 2, 4: 300 and 400 dpi",
    "format": [
      "TIFF",
      "JPEG"
    ],
    "reference": "8270155",
    "description": "\nThe ICFHR16 Competition \\cite{7814129} provided a collection of grayscale Latin manuscript images for script type classification.\nTwo tasks were proposed in this competition: Task 1, which uses a single label for each image, and Task 2, which uses multi weighted labeling for each image. \nThe dataset is comprised of 12 script classes and 3 sets, the training set contains 2K images and the test sets for Tasks 1 and 2 contain 1K and 2K images, respectively. \nThe average \\ac{acc} and the average intraclass distance were used for the evaluation of the proposed systems. The system that achieved the highest accuracy for Task 1 used I-vector extraction \\cite{Dehak2011LanguageRV} on image patches and a classification of the extracted vectors using Latent Dirichlet Analysis (LDA). \nFor Task 2, the best performing system in terms of Final Score utilized a neural network architecture named DeepScript\\footnote{\\url{https://github.com/mikekestemont/DeepScript}\\label{deepscript}} and pre-processing to yield information on random image crops and their various perturbations to the network classifier as input.\nIn terms of the average intraclass distance, the higher ranked system in both tasks was the FRDC-OCR and consisted of a \\ac{CNN} classifier that was applied on patches, where for every image, the result is the average of the recognition confidence and feature vector across its patches."
  },
  {
    "id": "569c8082",
    "name": "ScribbleLens",
    "statistics": "1K pages 28, 255 lines 281, 914 characters 85 writers",
    "class": "Writter-id and 150 years of origin classes",
    "task": [
      "Text recognition"
    ],
    "language": [
      "Dutch"
    ],
    "document_type": "Early Modern Manuscripts",
    "mode": [
      "Color",
      "Binary",
      "Grayscale"
    ],
    "resolution": "150 - 300 dpi",
    "format": [
      "JPEG"
    ],
    "reference": "9257750",
    "description": "\nIn \\cite{9257750}, a corpus for automatic manuscript transcription was presented. \nThis dataset contains 1K pages from early modern Dutch manuscripts spanning over 150 years with line, character, year, and writer ground truth. \nIt further provides a set of unlabeled images for unsupervised or weakly-supervised learning investigation. \nAs a baseline, a network that combines Convolutional Neural Networks and bi-directional \\ac{LSTM} with a \\ac{CTC} (CNN/BLSTM/\\ac{CTC}) \\cite{7814068, Nina2018NephiA} was used, and it was shown that the \\ac{CER} would be reduced in the presence of additional annotated data.\n "
  },
  {
    "id": "d43189c4",
    "name": "Amharic Database",
    "statistics": "Character dataset: 80K character images, 231 characters Text-line dataset: 40, 929 printed Power Geez text-line images, 197, 484 synthetic Power Geez text-line images, 98, 924 synthetic Visual Geez text-line images, 280 unique Amharic characters",
    "class": "280 unique Amharic characters",
    "task": [
      "Text recognition",
      "Binarization",
      "Text-line segmentation"
    ],
    "language": [
      "Amharic"
    ],
    "document_type": "Printed synthetic Amharic socuments",
    "mode": [
      "Grayscale"
    ],
    "resolution": "300 dpi Characters: 32\u00d732 pixels Text-lines: 48\u00d7128 pixels",
    "format": [
      "PNG"
    ],
    "reference": "8977980",
    "description": "\nThe Amharic database \\cite{8977980} presents a collection of 40,929 printed images with Amharic script text lines originating from pages of different documents written in Amharic and 296,403 synthetic images created using OCRopus \\cite{Breuel2008TheOO}.\nThe generated synthetic images include the Power Geez and the Visual Geez fonts.\nA Bidirectional \\ac{LSTM} followed by a softmax function that produced 281 probability values, which is the number of unique characters in the database, and a \\ac{CTC} output layer were proposed for text-line recognition.\nThis method achieved an 8.54\\% \\ac{CER} for the printed Power Geez documents, 2.28\\% \\ac{CER} for the Visual Geez synthetic images and 4.24\\% \\ac{CER} for the Power Geez synthetic images."
  },
  {
    "id": "0d718783",
    "name": "ICFHR18 RASM2018",
    "statistics": "10-15 train images 50-80 test images",
    "class": "Text, Graphic, Text line, Printed",
    "task": [
      "Page segmentation",
      "Text-line segmentation",
      "Text recognition"
    ],
    "language": [
      "Arabic"
    ],
    "document_type": "Arabic scientific manuscripts from 8nth-9nth centuries CE",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "TIFF"
    ],
    "reference": "8583806",
    "description": "\nThe RASM 2018 competition \\cite{8583806} was part of ICFHR 2018 and targeted the recognition of Arabic historical scientific manuscripts through three tasks: page segmentation, text-line detection, and \\ac{OCR}. \nAn example set of 15 single-column page images with PAGE XML ground truth format \\cite{5597587} was provided for training and 85 for evaluation to handle these tasks.\nAs in similar competitions, the ground truth included polygon regions, text transcriptions, and metadata for each region, such as headings, paragraphs, captions, footers, and reading order.\nFor system evaluation, the competition used the success rate and errors for the page and text-line segmentation predictions and the \\ac{ca} for the \\ac{OCR}.\nFor page segmentation, the winning system used an \\ac{FCN} applied on extracted patches.\nThe page layout predictions were further cropped for the text-line segmentation, which was performed at pixel-level using anisotropic Gaussian smoothing. \nThe highest performance was achieved for the rest of the tasks by a Historical Arabic Handwritten/Typewritten \\ac{OCR} system framework.\nThis system can handle various fonts and layouts, and in the case of the competition, an instance-based segmentation on extracted lines was performed."
  },
  {
    "id": "90133f35",
    "name": "OHG",
    "statistics": "596 pages, 23700 lines, 2400 words",
    "class": "6 layout regions, Handwritten",
    "task": [
      "Layout analysis",
      "Text recognition"
    ],
    "language": [
      "Spanish"
    ],
    "document_type": "Spanish notarial deeds of the 19nth century",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "Quiros2018",
    "description": "\nThe OHG dataset\\footnote{\\url{https://zenodo.org/record/1322666\\#.Ypi6Ty8RoUE}\\label{OHG_link}} is a set of 596 pages of Spanish deeds written from a single writer on the \\nth{18} century with a complex layout and six different layout regions that contain only text which are: page number, notarial typology, paragraph of text that begins next to a notarial typology, paragraph that begins on a previous page, marginal note, and marginal note added a posteriori to the document.\nThe dataset includes more than 23,700 lines and a 2,400-word vocabulary and the PAGE XML ground truth files for both layout analysis and handwritten recognition."
  },
  {
    "id": "50651e8f",
    "name": "Hugin-Munin",
    "statistics": "828 pages, 23, 732 lines 164, 922 words 752, 080 characters",
    "class": "12 writers",
    "task": [
      "Text recognition"
    ],
    "language": [
      "Norwegian"
    ],
    "document_type": "Pages from private correspondences and diaries from 12 writers written between 1820-1950",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "N/A"
    ],
    "reference": "10.1007/978-3-031-06555-2_27",
    "description": "\nThe Hugin-Munin dataset \\cite{10.1007/978-3-031-06555-2_27} is the first handwritten recognition dataset for text written in Norwegian.\nThis dataset contains images derived from diaries and private correspondences written from 1820 to 1950 from 12 different writers.\nThe ground truth includes the transcriptions of 164,922 words or 23,732 lines in PAGE XML format.\nThe authors provide a 80\\% training - 10\\% validation - 10\\% test random split and another split with 3 unseen writers in the test set.\nThey further present a survey of open-source handwritten text recognition libraries used since 2019 and compare the performance using the \\ac{CER} and \\ac{WER} on the random split data.\nThe lowest \\ac{CER} is obtained using PyLaia \\cite{puigcerver2018pylaia}, while the lowest \\ac{WER} is obtained using Kaldi \\cite{Arora2019UsingAM}.\nThese best methods were further deployed on the writer split and achieve much lower performance than the random split. "
  },
  {
    "id": "3e22e216",
    "name": "IAM-HistDB George Washington (GW)",
    "statistics": "20 pages, 656 lines, 4894 words",
    "class": "1, 471 word labels 82 letters, Handwritten",
    "task": [
      "Text recognition",
      "Word spotting"
    ],
    "language": [
      "English"
    ],
    "document_type": "Page images of a 18nth century manuscript written in longhand script by 2 writers",
    "mode": [
      "Binary"
    ],
    "resolution": "300 dpi",
    "format": [
      "PNG"
    ],
    "reference": "FISCHER2012934",
    "description": "\nIAM-HistDB \\cite{10.1145/1815330.1815331} is a highly used database of handwritten historical manuscript images that contains three datasets: Saint Gall, Parzival, and George Washington (GW).\nWe present these datasets in the following paragraphs.\n%All datasets contain Latin, Old German, and English text pages from the 9th, 13th and 18th century, respectively.\nThe Saint Gall database \\cite{10.1145/2037342.2037348} is a set of 60 page images and 1,410 binarized and normalized text-line images of manuscripts written in the \\nth{9} century in Latin language and Carolingian script by one writer.\nThe text edition for every page image was provided.\nThe pages are composed of 11,597 words, 4,890 word labels, 5,436 word spellings, and 49 letters.\nThe ground truth includes the line-level text transcriptions and the word and line locations.\nAn evaluation of a transcription alignment system based on HMM is proposed in the paper and compared with three more reference systems.\nThe Parzival database \\cite{FISCHER2012934} provides handwritten documents from the \\nth{13} century originating from three writers and was written in Old German and Gothic script.\nIt contains 47 pages, 4,477 text lines, 23,478 words, 4,934 word categories, and 93 letters.\nSimilar to St. Gall, the line and word images are binarized and normalized.\nAs ground truth, Parzival includes line- and word-level transcriptions.\nThe work presented in \\cite{5306020} used a HMM-based system similar to \\cite{10.5555/505741.505745} and the BLSTM introduced in \\cite{Graves2009855} recognizer for automatic handwriting recognition on the Parzival dataset and achieved a word \\ac{acc} of 88.69\\% and 93.32\\%.\nFurthermore, in \\cite{FISCHER2012934}, a lexicon-free word spotting method based on character HMMs was proposed and evaluated on the Parzival and GW datasets.\nThe GW database \\cite{FISCHER2012934} is comprised of \\nth{18} century documents from the George Washington Papers and contains 656 text and 4,894 word images, binarized and normalized, along with their transcription annotations.\nThe pages are written in English by two writers in longhand script.\nThe dataset statistics also include 1,471 word classes and 82 letters.\nThis dataset is widely used to evaluate word spotting algorithms. \n\\cite{5871643} used this database and compared a proposed word spotting method that used a BLSTM and a modified CTC algorithm with a HMM \\cite{10.1016/j.patcog.2009.02.005} and a DTW \\cite{Rath2006WordSF} method.\nThe paper presented average precision results using GW and Parzival datasets and the proposed method achieved 0.84 on the GW and 0.94 average precision on Parzival."
  },
  {
    "id": "8cdb2ee1",
    "name": "DIVA-HisDB",
    "statistics": "150 images, 493000000 pixels",
    "class": "Background Main text Decorations Comments",
    "task": [
      "Layout analysis",
      "Element extraction"
    ],
    "language": [
      "Latin",
      "Italian"
    ],
    "document_type": "Pages from 3 medieval manuscripts from 11nth and 14nth century",
    "mode": [
      "Color"
    ],
    "resolution": "600 dpi",
    "format": [
      "JPEG"
    ],
    "reference": "7814109",
    "description": "\nDIVA-HisDB \\cite{7814109} is a database that contains 150 images derived from three medieval manuscripts from the \\nth{11} and \\nth{14} centuries with complex layouts.\nThis database provides 20 training, 10 validation, 10 test, and 10 left out images for every manuscript annotated at pixel-level using the PAGE format for the following classes: main text body, decorations, and comments.\nBenchmark results applying convolutional autoencoders (N-light-N) \\cite{7814107} showed an average accuracy of approximately 95\\% for pixel classification and the accuracy for every category. \nAdditionally, the challenges of the dataset in terms of text-line segmentation using the Seam Carving \\cite{6981106} and OCRopus \\cite{Breuel2008TheOO} methods were demonstrated.\nThe HisDoc-Layout-Comp competition of ICDAR 2017 \\cite{8270154} used the DIVA-HisDB dataset to evaluate systems on layout analysis (Task 1), baseline detection (Task 2), and text-line segmentation (Task 3).\nFor layout analysis, the best overall performance in terms of \\ac{mIoU} was achieved by a \\ac{FCN} that segmented every image at pixel-level.\nThe best performing system for Tasks 2 and 3 deployed Adaptive Run Length Smoothing (ARLS) \\cite{Nikolaou2010SegmentationOH} to propose text lines and then processed them using the Seam Carving algorithm."
  },
  {
    "id": "8d526da2",
    "name": "BiblIA",
    "statistics": "202 images 11, 285 lines 74, 675 words",
    "class": "Script classes: Ashkenazi Italian, Sephardi",
    "task": [
      "Text-line segmentation",
      "Text recognition"
    ],
    "language": [
      "Hebrew",
      "Aramaic"
    ],
    "document_type": "Hebrew Manuscripts written from 11nth-15nth",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "10.1145/3476887.3476892",
    "description": "\nBiblIA \\cite{10.1145/3476887.3476896} is a publicly available dataset of Medieval manuscripts written in Hebrew and Aramaic that contains 6 different scripts: Ashkenazi, Byzantine, Italian, Oriental, Sephardi, Yemenite.\nBiblIA contains more than 200 images with their corresponding annotations on baseline- and transcription-level, both focusing on the main text.\nFurthermore, a segmentation and recognition model based on kraken OCR is used for evaluation.\nThe work presents \\ac{acc} results on specific scripts (Ashkenazi, Italian, and Sephardi) and all scripts as well as training information.\nFurther experimental results show the \\ac{CER} and \\ac{WER} on images not included in the test set."
  },
  {
    "id": "f904bf13",
    "name": "Digital Peter",
    "statistics": "9, 694 text-line images (6, 237 train, 1, 930 validation, 1, 527 test) 265, 788 characters 50, 998 words",
    "class": "N/A",
    "task": [
      "Text-line segmentation",
      "Text recognition"
    ],
    "language": [
      "Russian"
    ],
    "document_type": "Manuscripts from Peter the Great written from 1709-1713",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "10.1145/3476887.3476892",
    "description": "\nDigital Peter \\cite{10.1145/3476887.3476892} is a dataset of 9,694 images and their corresponding text from manuscripts, written by Peter the Great from 1709 to 1713 for handwriting recognition.\nThis dataset provides a 6,237 training, 1,930 validation, and 1,527 test splits that can be used either for line segmentation or line recognition.\nA competition\\footnote{\\url{https://github.com/sberbank-ai/digital\\_peter\\_aij2020}\\label{digital_peter_note}} on text-line recognition was launched using this dataset.\nAs a baseline, a 7-layer \\ac{CNN} was used for image feature extraction and then a bidirectional GRU network with \\ac{CTC} loss \\cite{10.1145/1143844.1143891} was used to predict the image text.\nThe model performance was further optimized using different hyperparameter values and beam search.\nThe task was evaluated according to the \\ac{CER}, the \\ac{WER}, and the string \\ac{acc}."
  },
  {
    "id": "67e9c0c7",
    "name": "ESPOSALLES INDEX",
    "statistics": "29 pages, 1563 lines, 6534 words, 30809 characters",
    "class": "1, 725 lexicon size 68 character classes, Handwritten",
    "task": [
      "Text recognition"
    ],
    "language": [
      "Spanish"
    ],
    "document_type": "Handwritten documents from marriage license books",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "ROMERO20131658",
    "description": "\nThe ESPOSALLES database \\cite{ROMERO20131658} is a collection of ancient marriage license documents separated into the LICENSES and the INDEX subsets.\nA single-writer book written in old Catalan is the main content of the LICENSES set, and is comprised of 173 pages and 1,747 licenses.\nFor every page, the subset includes the main text block bounding box, the text line within the text block coordinates, the license label, and the transcription for every line, word, and character in the main block.\nThe INDEX subset, which includes 29 pages of the initial indexes of two volumes by a single writer created between 1491 and 1495.\nSimilar to the LICENSES part, INDEX contains text and line layout as well as transcription annotations.\nBoth subsets provide dataset splits for cross-validation.\nFinally, baseline results using Hidden Markov Models (HMM) \\cite{Toselli2004IntegratedHR} and a BLSTM \\cite{Graves2009855} with two feature sets, PRHLT \\cite{Toselli2004IntegratedHR} and IAM \\cite{10.5555/505741.505745}, showed the efficiency of neural networks with larger datasets for handwriting recognition.\nThe database was further used in the ICDAR17 Competition on Information Extraction in Historical Handwritten Records \\cite{8270158}.\nThe aim there was to detect and assign name entities to semantic categories (name, surname, occupation, etc.) for two Tracks: Basic and Complete, which also contains the person (husband, wife, etc.).\nThe team that obtained the highest average score with word-level segmentation used a ResNet-based unigram system for character recognition and named entity recognition, while with line-level segmentation, the best method used a RNN-LSTM with \\ac{CTC}."
  },
  {
    "id": "cb71fc40",
    "name": "ICDAR19 DMAS2019",
    "statistics": "50-100 PAGE annotated images",
    "class": "Page classes: cover table of contents content index Article classes: article illustration with caption advertisement index colophon",
    "task": [
      "Article segmentation",
      "Page segmentation",
      "Text recognition"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Digitised image pages from magazines from 1800 - 1938 by the National Library of the Netherlands",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG2000"
    ],
    "reference": "",
    "description": "\nThis competition\\footnote{\\url{https://www.primaresearch.org/DMAS2019/}\\label{dmas2019}} aimed to recognize and classify parts of articles present in digitized historical magazines.\nThe competition provided 50-100 annotated images from magazines from 1800 -1938 taken by the National Library of the Netherlands and their layout and \\ac{OCR} ground truth.\nThe annotations include cover, table of contents, content, and index as page classes and article, illustration with caption, advertisement, index, and colophon as article classes.\nThe competition does not seem to provide information about the submitted systems."
  },
  {
    "id": "33ea38fb",
    "name": "SleukRith",
    "statistics": "301626 characters, 73359 words, 3245 lines",
    "class": "207 character classes 6, 284 unique words",
    "task": [
      "Character recognition",
      "Word segmentation",
      "Text-line segmentation"
    ],
    "language": [
      "Khmer"
    ],
    "document_type": "Palm leaf manuscript pages",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "10.1145/3151509.3151510",
    "description": "\nSleukRith \\cite{10.1145/3151509.3151510} is a dataset of 657 images from palm leaf manuscripts written in Khmer from 4 different sources.\nThis dataset includes annotations for isolated character recognition and word and line segmentation.\nThe most valuable aspect of this dataset is character recognition, which is the foundation for building the other two elements.\nThe individual character images were constructed by cutting patches for every character and removing the noise of near characters using inpainting.\nFor the rest of the annotations, the combination of the characters was used to determine the words and lines.\nTo evaluate the set for character recognition, the \\ac{CER} of a \\ac{CNN}, which was 6.04\\%, was presented.\nThe dataset was further used in the ICFHR2018 Competition for Southeast Asian Palm Leaf Manuscripts presented in Section \\ref{sssec:ICFHR18_ASIAN_PALM_LEAF}, which contained binarization, text-line segmentation, character recognition, and word transliteration tasks.\nHowever, this dataset was not part of the binarization task.\nThe winning systems presented in the competition section also performed the best for this dataset alone."
  },
  {
    "id": "d4ed8582",
    "name": "ENP",
    "statistics": "528 pages, 61K regions, 208 tables, 1000 graphics, 47000 text_regions, 202000 lines",
    "class": "Regions, Tables Images/Graphics Text regions, Text lines",
    "task": [
      "Text recognition",
      "Layout analysis"
    ],
    "language": [
      "Dutch",
      "English",
      "Estonian",
      "Finnish",
      "French",
      "German",
      "Latvian",
      "Polish",
      "Russian",
      "Serbian",
      "Swedish",
      "Ukrainian",
      "Yidish"
    ],
    "document_type": "European Cultural Heritage",
    "mode": [
      "Color",
      "Grayscale",
      "Binary"
    ],
    "resolution": "300 dpi 400 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "7333898",
    "description": "\nThe Europeana Newspapers Project (ENP) dataset \\cite{7333898} consists of European cultural heritage documents of 13 different languages from 12 European libraries published in newspapers from the \\nth{17} to the \\nth{20} century. \nAll page images in the dataset are either 300 or 400 dpi, and there is a broad distribution of grayscale, bitonal, and color pages. \nThe ground truth contains region outlines and their labels (text lines, text regions, tables, images/graphics, and blocks/zones), Unicode text transcriptions, and reading order.\nIn addition to conventional downloading, this dataset can be accessed through a web interface with several services such as document and attachment retrieval and a search function to determine if a document exists in the database.\nA performance evaluation for \\ac{OCR} and layout analysis was conducted using a commercial system (ABBYY FineReader 11) and an OCR system (Tesseract 3.03)\\footnote{\\url{https://github.com/tesseract-ocr/tesseract}\\label{tesseract}}."
  },
  {
    "id": "ee8bd3b2",
    "name": "ICDAR19 DIBCO 2019",
    "statistics": "10 historical machine-printed and handwritten test images 10 papyri test images",
    "class": "N/A",
    "task": [
      "Binarization"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Historical machine-printed and handwritten images of the 19nth century and papyri images from various periods of Antiquity",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      "BMP"
    ],
    "reference": "8978205",
    "description": "\nThe latest of the DIBCO competition series of 2019 \\cite{8978205} aimed in evaluating various systems for the task of image binarization.\nThe series of this competition initiated in 2009 \\cite{5277767} and had several rounds for printed and handwritten document images \\cite{8270159, 6628857, 6065249}.\nThe 2019 competition included two categories, CATEGORY I, that provided 10 historical handwritten and printed test images of the \\nth{19} century, and CATEGORY II, that provided 10 test images derived from papyri of various places in Egypt.\nFor CATEGORY I, the best performing method used noise reduction and then an ensemble of three clustering algorithms (Fuzzy C-Means, K-Medoids and K-Means++) for the step of grouping the foreground and background of the input images.\nThe best performing system for CATEGORY II, used the neural network architecture LadderNet \\cite{Zhuang2018LadderNetMN} on $48\\times48$ image patches.\nAll systems were evaluated using \\ac{FM}, pseudo-\\ac{FM} ($F_{ps}$), PSNR, and Distance Reciprocal Distortion Metric (DRD)."
  },
  {
    "id": "59d470f7",
    "name": "Multiple Tripitaka in Han (MTH)",
    "statistics": "MTH 500 images 17178 lines 197886 characters",
    "class": "3664 Character classes",
    "task": [
      "Character segmentation",
      "Character recognition"
    ],
    "language": [
      "Chinese"
    ],
    "document_type": "Chinese historical documents",
    "mode": [
      "N/A"
    ],
    "resolution": "N/A",
    "format": [
      "N/A"
    ],
    "reference": "Yang2018DenseAT",
    "description": "\nThe work presented in \\cite{Yang2018DenseAT} introduces two datasets for Chinese character detection and recognition, the Tripitaka Koreana in Han (TKH) and the Multiple Tripitaka in Han (MTH), created using the publicly available TKH images from the Tripitaka Koreana Institute.\n%The images were annotated by detecting the text lines and then segmenting them using beam-search to obtain the bounding box annotations.\nFor every character bounding box that the dataset includes, a character label was further provided.\nThe TKH consists of 1K pages, 23,471 lines, 323,491 characters, and 1,471 character classes, while the MTH contains 500 images, 17,178 lines, 197,886 characters, and 3,664 character classes.\nThe two datasets differ in terms of challenge, as the MTH dataset has a more complex character size uniformity, making the creation of bounding box annotations even harder.\nA three-part pipeline called Recognition Guided Detector (RGD) was proposed. \nFirst, segmentation is performed for every line. \nA Recognition Guided Proposal Network (RGPN) generates context information, and finally, a detector uses that information to find the characters in every line.\nFinally, several experiments were performed on the two datasets using the proposed system with and without a VGG-16 \\cite{Simonyan2015VeryDC} backbone and its performance was compared with other well-known object detection frameworks using either the whole image or text lines as input.\nThis method seems to perform comparably to other methods using fewer parameters."
  },
  {
    "id": "49e86f02",
    "name": "FCR",
    "statistics": "500 pages",
    "class": "6 layout regions, Handwritten",
    "task": [
      "Layout analysis",
      "Text recognition"
    ],
    "language": [
      "Swedish"
    ],
    "document_type": "Pages of records of deeds mortgages traditional life-annuity from the Renovated District Court Records (19nth century)",
    "mode": [
      "Grayscale"
    ],
    "resolution": "N/A",
    "format": [
      ""
    ],
    "reference": "Quirs2020FinnishCR",
    "description": "\nThe FCR dataset \\cite{Quirs2020FinnishCR} includes 500 pages from the Renovated District Court\nRecords of Finland from the \\nth{19} century.\nThe images are both single- or double-page which makes the dataset quite complex and the corresponding ground truth contains annotations on baseline- and layout-level.\nThe layout regions included are: page number, marginalia, paragraph, paragraph2, table, and table2.\nThe ground truth further includes the line-level transcriptions in the Swedish language."
  },
  {
    "id": "c75447f7",
    "name": "ARDIS",
    "statistics": "\\textit{Dataset I}: 10K 4-digit images \\textit{Dataset II - IV}: 7.6K digit images (6.6K train/1K test)",
    "class": "\\textit{Dataset I}: 75 year classes \\textit{Dataset II - IV}: 10 classes (0-9), Handwritten",
    "task": [
      "Character recognition"
    ],
    "language": [
      "Swedish",
      "Latin"
    ],
    "document_type": "Swedish handwritten document crops of digits from 1800 - 1940",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "\\textit{Dataset I}: 175\u00d795 pixels \\textit{Dataset IV}: 28\u00d728 pixels",
    "format": [
      "JPEG"
    ],
    "reference": "kusetogullari2020ardis",
    "description": "\nArkiv Digital Sweden (ARDIS) \\cite{kusetogullari2020ardis} is a handwritten digit dataset collection derived from historical church records. \nARDIS contains four different datasets: \\textit{Dataset I}, which contains 10K 4-digit string images that represent a year, \\textit{Dataset II}, which contains single digits of classes 0-9, \\textit{Dataset III}, which is the same as \\textit{Dataset II} but is cleansed from noise, and \\textit{Dataset IV}, which is the same as \\textit{Dataset III} but is in grayscale and is similar to the highly used MNIST Database \\cite{lecun1998mnist}.\n\\textit{Dataset II - IV} contain 7,600 digit images.\nSeveral experiments using CNN, SVM, HOG+SVM, k-NN, random forest, and RNN classifiers were presented. \nThe results of training using the MNIST and USPS \\cite{291440} datasets and testing on ARDIS reveal the diversity of the ARDIS dataset as the highest \\ac{RA} obtained reaches $58.80\\%$, while training and testing on ARDIS gives a performance of $98.6\\%$.\nIn all experimental cases, the best performance was achieved by the \\ac{CNN} digit classifier."
  },
  {
    "id": "64ab0d69",
    "name": "Pinkas",
    "statistics": "30 pages, 1013 lines, 13744 words",
    "class": "3, 117 train classes 1, 251 test classes main text line and word segmentation class",
    "task": [
      "Word spotting",
      "Page segmentation"
    ],
    "language": [
      "Hebrew"
    ],
    "document_type": "Manuscript records of Jewish communities in Europe from 1500-1800",
    "mode": [
      "Color"
    ],
    "resolution": "High-resolution",
    "format": [
      "JPEG"
    ],
    "reference": "8978129",
    "description": "\nThe Pinkas dataset \\cite{8978129} is a collection of 30 handwritten medieval Hebrew pages intended for page, line, and word segmentation tasks. \nThe training and test sets contain 10,397 and 3,278 images, respectively.\nThese images are derived from records of European Jewish communities from 1500 to 1800.\nThe \\ac{mAP} of different word spotting methods, including \\ac{CNN} variations, was utilized in this study.\nThree methods were set to provide a baseline for the dataset.\nSiamese \\ac{CNN} \\cite{Bromley1993SignatureVU} and PHOCNet \\cite{Sudholt2016PHOCNetAD} were compared as segmentation methods and an SVM with HOG descriptors \\cite{Almazn2012EfficientEW} was used as a segmentation-free method.\nThe Siamese CNN, which achieved a \\ac{mAP} of 61.5\\%, outperformed the other methods. PHOCNet achieved a \\ac{mAP} of 53.3\\% using one-hot encoding and 56.6\\% without it. \nThe SVM did not perform well, as it achieved a \\ac{mAP} of 1.5\\%."
  },
  {
    "id": "9b5ab6e5",
    "name": "GRK-Papyri PapyRow",
    "statistics": "GRK-Papyri: 50 images PapyRow: 6, 498 samples",
    "class": "10 writers",
    "task": [
      "Writer identification",
      "Image enhancement",
      "Binarization",
      "Text-line segmentation",
      "Word segmentation"
    ],
    "language": [
      "Greek"
    ],
    "document_type": "Handwritten Greek papyri from the 6nth century A.D.",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "GRK-Papyri: 96-2000 dpi Height: 796-6818 Width: 177-2000 PapyRow: 1200 pixels width 500 pixels width",
    "format": [
      "JPEG"
    ],
    "reference": "8978142",
    "description": "\nThe GRK-Papyri dataset \\cite{8978142} provides 50 handwritten Greek papyrus images from the \\nth{6} century A.D for writer identification. It includes color and grayscale documents with 4-7 samples each from 10 different writers. \nThis dataset provides a leave-one-out option that contains all images without any split and a train-test split with a balanced training set of 20 samples, keeping the rest with different numbers of samples per writer for testing.\nThe dataset has a high complexity, as the images are heavily degraded and low in quality, making pre- and post-processing inevitable.\nDue to size limitations, the method used for evaluation was a Normalized Local Na\\\"{\\i}ve Bayes Nearest-Neighbour (NBNN) classifier with FAST keypoints \\cite{mohammed2017normalised}.\nThe authors suggested that this dataset can be further used for image processing tasks or line/word segmentation.\nAn extension of the GRK-Papyri is presented in \\cite{10.1007/978-3-030-68787-8_16}, where enhancement techniques, such as background smoothing, line resizing, and image rotation,  were used to obtain images with less degradation.\nIn this extended version of the dataset, named PapyRow, 6,498 images were obtained using a row segmentation method and included with their corresponding XML ground truth."
  },
  {
    "id": "50840302",
    "name": "ICFHR 2016 CLAMM",
    "statistics": "2K training images Task 1: 1K test images Task 2: 2K test images",
    "class": "Uncial, Half-uncial, Caroline, Humanistic Humanistic Cursive, Praegothica, Southern Textualis, Semitextualis, Textualis, Hybrida Semihybrida, Cursiva (12 classes)",
    "task": [
      "Script type classification"
    ],
    "language": [
      "Latin"
    ],
    "document_type": "Manuscripts from the French catalogues BVMM and Gallica",
    "mode": [
      "Grayscale"
    ],
    "resolution": "300 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "7814129",
    "description": "\nThe ICFHR16 Competition \\cite{7814129} provided a collection of grayscale Latin manuscript images for script type classification.\nTwo tasks were proposed in this competition: Task 1, which uses a single label for each image, and Task 2, which uses multi weighted labeling for each image. \nThe dataset is comprised of 12 script classes and 3 sets, the training set contains 2K images and the test sets for Tasks 1 and 2 contain 1K and 2K images, respectively. \nThe average \\ac{acc} and the average intraclass distance were used for the evaluation of the proposed systems. The system that achieved the highest accuracy for Task 1 used I-vector extraction \\cite{Dehak2011LanguageRV} on image patches and a classification of the extracted vectors using Latent Dirichlet Analysis (LDA). \nFor Task 2, the best performing system in terms of Final Score utilized a neural network architecture named DeepScript\\footnote{\\url{https://github.com/mikekestemont/DeepScript}\\label{deepscript}} and pre-processing to yield information on random image crops and their various perturbations to the network classifier as input.\nIn terms of the average intraclass distance, the higher ranked system in both tasks was the FRDC-OCR and consisted of a \\ac{CNN} classifier that was applied on patches, where for every image, the result is the average of the recognition confidence and feature vector across its patches."
  },
  {
    "id": "baf195d7",
    "name": "IAM-HistDB Saint Gall",
    "statistics": "60 pages, 1410 lines, 11597 words, 5436 word_spellings",
    "class": "4, 890 word labels 49 letters, Handwritten",
    "task": [
      "Text recognition",
      "Layout analysis"
    ],
    "language": [
      "Latin"
    ],
    "document_type": "Page images of a 9nth century manuscript written in Carolingian script by a single writer",
    "mode": [
      "Color"
    ],
    "resolution": "300 dpi",
    "format": [
      "JPEG",
      "PNG"
    ],
    "reference": "10.1145/2037342.2037348",
    "description": "\nIAM-HistDB \\cite{10.1145/1815330.1815331} is a highly used database of handwritten historical manuscript images that contains three datasets: Saint Gall, Parzival, and George Washington (GW).\nWe present these datasets in the following paragraphs.\n%All datasets contain Latin, Old German, and English text pages from the 9th, 13th and 18th century, respectively.\nThe Saint Gall database \\cite{10.1145/2037342.2037348} is a set of 60 page images and 1,410 binarized and normalized text-line images of manuscripts written in the \\nth{9} century in Latin language and Carolingian script by one writer.\nThe text edition for every page image was provided.\nThe pages are composed of 11,597 words, 4,890 word labels, 5,436 word spellings, and 49 letters.\nThe ground truth includes the line-level text transcriptions and the word and line locations.\nAn evaluation of a transcription alignment system based on HMM is proposed in the paper and compared with three more reference systems.\nThe Parzival database \\cite{FISCHER2012934} provides handwritten documents from the \\nth{13} century originating from three writers and was written in Old German and Gothic script.\nIt contains 47 pages, 4,477 text lines, 23,478 words, 4,934 word categories, and 93 letters.\nSimilar to St. Gall, the line and word images are binarized and normalized.\nAs ground truth, Parzival includes line- and word-level transcriptions.\nThe work presented in \\cite{5306020} used a HMM-based system similar to \\cite{10.5555/505741.505745} and the BLSTM introduced in \\cite{Graves2009855} recognizer for automatic handwriting recognition on the Parzival dataset and achieved a word \\ac{acc} of 88.69\\% and 93.32\\%.\nFurthermore, in \\cite{FISCHER2012934}, a lexicon-free word spotting method based on character HMMs was proposed and evaluated on the Parzival and GW datasets.\nThe GW database \\cite{FISCHER2012934} is comprised of \\nth{18} century documents from the George Washington Papers and contains 656 text and 4,894 word images, binarized and normalized, along with their transcription annotations.\nThe pages are written in English by two writers in longhand script.\nThe dataset statistics also include 1,471 word classes and 82 letters.\nThis dataset is widely used to evaluate word spotting algorithms. \n\\cite{5871643} used this database and compared a proposed word spotting method that used a BLSTM and a modified CTC algorithm with a HMM \\cite{10.1016/j.patcog.2009.02.005} and a DTW \\cite{Rath2006WordSF} method.\nThe paper presented average precision results using GW and Parzival datasets and the proposed method achieved 0.84 on the GW and 0.94 average precision on Parzival."
  },
  {
    "id": "d9e121e0",
    "name": "DIGIDOC-Texture",
    "statistics": "%",
    "class": "",
    "task": [
      ""
    ],
    "language": [
      "%"
    ],
    "document_type": "",
    "mode": [
      ""
    ],
    "resolution": "300 dpi",
    "format": [
      ""
    ],
    "reference": "Mehri2016TextureFB",
    "description": "\n%This paper \\cite{Mehri2016TextureFB} investigates and benchmarks texture features extracted from 9 texture-based algorithms.\n%To this end, they present the DIGIDOC-Texture dataset that is collected from different types of books found in Gallica.\n%The dataset contains 1,000 high resolution page images that contain (1) one font and graphics, (2) two fonts and graphics, (3) two fonts, or (4) three fonts."
  },
  {
    "id": "9efee761",
    "name": "DIDA",
    "statistics": "Dataset I: 250K single digits Dataset II: 200K multi-digits Dataset III: 25K bounding boxes",
    "class": "Dataset I: 0 - 9, Handwritten",
    "task": [
      "Character recognition",
      "Character segmentation"
    ],
    "language": [
      "Swedish"
    ],
    "document_type": "Swedish handwritten document crops from 1800 - 1940",
    "mode": [
      "Color"
    ],
    "resolution": "240\u00d7 210 Pixels",
    "format": [
      "JPEG"
    ],
    "reference": "KUSETOGULLARI2021100182",
    "description": "\nThe Digit Dataset DIDA \\cite{KUSETOGULLARI2021100182} is an extension of the previously mentioned ARDIS digit dataset. \nDIDA is composed of three datasets: Dataset I, with 250K single-digit color images of 10 classes (0-9), Dataset II, with 200K multi-digit year string samples, and Dataset III, with 25K digits with bounding boxes meant for object detection.\nA digit detection and recognition system named DIGITNET was proposed that initially detects handwritten digits and passes the output to a recognition network to classify them.\nThe results of this system were further evaluated on DIDA, comparing it to other classical methods \\cite{merabti2018segmentation, Chen2000SegmentationOS,Gattal2017SegmentationAR} and network architectures such as YOLOv3 and YOLOv3-tiny \\cite{Redmon2018YOLOv3AI}. \nSimilar to ARDIS, several experiments with different combinations of datasets were performed and state-of-the-art results in digit detection were achieved."
  },
  {
    "id": "f07d28e5",
    "name": "HORAE",
    "statistics": "557 images, 797 pages, 843 text regions, 1112 line fillers, 12512 text-lines, 284 miniatures, 892 decorated boders, 118 illustrated borders, 2776 decorated initials, 551 simple initials, 22 historiated initials, 5 ornamentation, 4 music notations",
    "class": "Page classes: calendar miniature miniature-and- text text-with-miniature full-page text",
    "task": [
      "Text-line segmentation",
      "Layout analysis"
    ],
    "language": [
      "Latin"
    ],
    "document_type": "Pages from Books of Hours",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "10.1145/3352631.3352633",
    "description": "\nThe HORAE dataset \\cite{10.1145/3352631.3352633} contains 557 images derived from books of hours and their corresponding layout and text-related annotations.\nThese images originate from the full HORAE corpus, which consists of 500 manuscripts and 107,227 pages.\nTo create the final HORAE dataset with the annotated pages, a selection pipeline was used that initially classified pages into the following classes: binding, white page, calendar, miniature, miniature-and-text, text-with-miniature, and full-page text.\nThey excluded the ones that were binding or white pages and kept two images per class.\nThen, the filtered pages were clustered from the initial step to keep one from every class and detect the ones considered outliers because of their rare layout. \nFor the final 557 image set, the centroids from the most frequent layouts and the strongest outliers were annotated.\nA PAGE XML \\cite{5597587} file accompanies every image of the final set with annotations for page, miniature, border elements, initials, and other decorations found in the text body, such as line filler, music notations, and ornaments. \nBenchmark results were presented for line detection and layout analysis using the dhSegment segmentation neural network \\cite{Oliveira2018dhSegmentAG} and evaluated according to the IoU with different thresholds and postprocessing."
  },
  {
    "id": "e74772d3",
    "name": "ICDAR 2019 HDRC-IR",
    "statistics": "Train and validation from \\ref{sssec:Historical-WI} 20K test images",
    "class": "10K writers Writers from manuscript books letter or charters",
    "task": [
      "Writer identification"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Handwritten document pages of manuscript books letters charters and legal docs from various institutions",
    "mode": [
      "Color"
    ],
    "resolution": "High and low quality factor resolution 2K pixel larger dimension",
    "format": [
      "JPEG"
    ],
    "reference": "Christlein2019ICDAR2C",
    "description": "\nThis competition \\cite{Christlein2019ICDAR2C} followed the previous competition mentioned in \\ref{sssec:Historical-WI} and handled the task of image retrieval according to writer style by providing a larger test set of 20K images from over 10K different writers.\nFor training, the competition proposed the dataset from the previous competition and further enlarged the training with images from Letters A and Manuscripts.\nThe \\ac{mAP} constituted the evaluation metric, similar to the previous competition. \nThe winning system obtained a 92.5\\% \\ac{mAP} using SIFT \\cite{LoweDavid2004DistinctiveIF} and Pathlet features \\cite{8978107} projected into a lower dimension space using SVD on the ICDAR17 Historical-WI data feature matrices, and then concatenated and normalized them to compute global descriptors using Euclidean distance."
  },
  {
    "id": "40c05165",
    "name": "MHDID",
    "statistics": "335 images",
    "class": "Paper translucency Stain Reader's annotations Worn holes",
    "task": [
      "Distortion classification",
      "Visual quality evaluation"
    ],
    "language": [
      "Arabic"
    ],
    "document_type": "Book pages edited from 1nth - 14nth Islamic Centuries",
    "mode": [
      "Color"
    ],
    "resolution": "1024\u00d7 1280",
    "format": [
      "JPEG"
    ],
    "reference": "8480372",
    "description": "\nMHDID is the Multi-distortion Historical Document Image Database \\cite{8480372} for document quality assessment and distortion classification.\nThis dataset contains 335 images with four degradation types: wormholes, stains, reader annotations, and paper translucency.\nThe document images emanate from 130 books from the Qatar University Library and are written in Arabic.\nSeveral users are supposed to compare pairs of images and select among three options.\nThese options are \"The left image is better\", \"The images are similar\", or \"The right image is better\".\nWith six outliers removed, the user interface results were normalized between 0, the lowest perceptual quality value, and 9, the highest.\nFinally, the MOS value was computed for every image, which is the sum of the outcome pair comparisons divided by the number of pairs. \nA dataset analysis was further demonstrated in terms of color and spatial information to reveal the heterogeneity of the dataset. \nThis database seems to be an outlier. \nThus, we categorize it as retrieval in Table \\ref{tab:structure_content_class} with a (\\checkmark) since it is a database that compares pairs of images."
  },
  {
    "id": "4d7e134b",
    "name": "Multiple Font Groups",
    "statistics": "35623 images",
    "class": "Textura, Rotunda Gotico-antiqua, Hebrew Bastarda, Schwabacher Fraktur, Antiqua Italic, Greek Other Font, Not a Font",
    "task": [
      "Font classification"
    ],
    "language": [
      "Latin",
      "Greek",
      "Hebrew",
      "Other"
    ],
    "document_type": "Pages from early printed books from 15nth-18nth centuries",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "side length: 79-14K pixels median surface: 5.3\u00d710^6 pixels",
    "format": [
      "JPEG",
      "TIFF"
    ],
    "reference": "10.1145/3352631.3352640",
    "description": "\nThis dataset \\cite{10.1145/3352631.3352640} contains a set of 35,623 images for font classification and consists of 12 different classes: Antiqua, Italic, Textura, Rotunda, Gotico-Antiqua, Bastarda, Schwabacher, Fraktur, Greek, Hebrew, \"Other Fonts\" and \"Not a Font\".\nSeveral of the data samples have multiple labels, thus this dataset is appropriate for testing multilabel classification methods.\nThis dataset is considered highly imbalanced.\nBaseline results on ResNet with 50 and 18 layers \\cite{He2016DeepRL}, VGG with 16 layers \\cite{Simonyan2015VeryDC}, and DenseNet with 121 layers \\cite{huang2018densely} were presented, and the \\ac{mIoU} metric was reported.\nThis dataset was further used on the Historical Document Classification Competition \\cite{10.1007/978-3-030-86337-1_41} that was part of the \\nth{16} \\ac{ICDAR} 2021.\nFurther information about this competition is presented in \\ref{sssec:his_doc_classification_icdar_2021}."
  },
  {
    "id": "c63cd32a",
    "name": "PBOK",
    "statistics": "707 pages, 12565 lines, 104541 words, 553536 characters, 436 writers",
    "class": "4 languages, 436 writers",
    "task": [
      "Text-line segmentation",
      "Word segmentation",
      "Word recognition"
    ],
    "language": [
      "Persian",
      "Bangla",
      "Oriya",
      "Kannada"
    ],
    "document_type": "Handwritten documents",
    "mode": [
      "Grayscale"
    ],
    "resolution": "300 dpi",
    "format": [
      "TIFF"
    ],
    "reference": "Alaei2012DatasetAG",
    "description": "\nThe PBOK dataset \\cite{Alaei2012DatasetAG} includes images of 707 handwritten pages in four languages by numerous writers.\nMore specifically, this dataset provides 140 pages written in Persian, 228 pages written in Kannada, 199 pages written in Bangla, and 140 pages written in Oriya pages.\nThis dataset contains both pixel- and content-level annotations.\nPBOK is considered quite complex, as it contains handwriting in both directions (left to right and right to left) and overlapping text.\nThe authors conducted line segmentation experiments for each language part and for the whole dataset using two algorithms, the Potential Piecewise Separation Line (PPSL) \\cite{10.1007/s10044-011-0226-x} and the method proposed by Alaei et al. \\cite{Alaei2011ANS}.\nThe Alaei et al. method achieved a \\ac{DR} of 91.33\\%, a \\ac{RA} of 90.41\\%, and an overall segmentation result (TLDM) of 90.87\\%, and outperformed PPSL, which achieved values of 88.07\\%, 86.69\\%, and 87.38\\%, respectively.\n "
  },
  {
    "id": "85493345",
    "name": "Warped Arabic",
    "statistics": "200 pages",
    "class": "4 centuries 4 document types: book page newspaper legal document journal other document unclassified",
    "task": [
      "Text-line segmentation"
    ],
    "language": [
      "Arabic"
    ],
    "document_type": "Arabic historical documents from the 16nth-19nth century with curls and warping from 4 document types",
    "mode": [
      "Color"
    ],
    "resolution": "350 dpi",
    "format": [
      "TIFF",
      "JPEG"
    ],
    "reference": "iet:/content/conferences/10.1049/cp.2018.1286",
    "description": "\nIn \\cite{iet:/content/conferences/10.1049/cp.2018.1286}, a dataset of 200 historical Arabic document images from the 16th to the 19th century and four different libraries was introduced. \nThe images were derived from books, newspapers, legal documents, and journals and contain a variety of layouts and states of degradation that help in facing challenges in baseline detection and text-line segmentation tasks. \nThe PAGE XML ground truth \\cite{5597587} contains text line-level information and metadata information according to bibliographic knowledge (author, title, date, location, document type, and page number), physical properties (language, script, font, and number of columns), and copyright data. \nResults were shown for text-line segmentation using four methods: Voronoi diagrams, a smearing method, a hybrid approach, and a projection profile-based method. \nFor these methods, three warping percentages were utilized: 0\\%, 25\\%, and 50\\%. \nThe results suggested that the Voronoi diagrams achieved the highest success rate. Finally, the more warped the text lines are, the more challenging the task is. \nThus, the performance decreases as the curvature increases."
  },
  {
    "id": "c427ca34",
    "name": "ICFHR 2020 HisFragIR20",
    "statistics": "220K fragment images (101, 706 train - 20, 019 test)",
    "class": "9, 800K writers (8, 717 train - 1, 152 test)",
    "task": [
      "Writer identification"
    ],
    "language": [
      "N/A"
    ],
    "document_type": "Manuscript pages from books of the European Middle Ages 9nth - 15nth century CE",
    "mode": [
      "Color"
    ],
    "resolution": "High and low quality factor resolution 2K pixel larger dimension",
    "format": [
      "JPEG"
    ],
    "reference": "Seuret2020ICFHR2C",
    "description": "\nAnother competition, which is similar to the ICDAR17 Historical-WI (Section \\ref{sssec:Historical-WI}) and the ICDAR2019-HDRC-IR (Section \\ref{sssec:ICDAR2019-HDRC-IR}) is the HisFragIR20 \\cite{Seuret2020ICFHR2C}.\nThis competition further increased the size of the dataset by generating 120K fragments, randomly shaped and rectangular, from 20K documents and 9.8K writers.\nThe test data come from European Middle Age books (\\nth{9} to \\nth{15} century CE).\nFragments extracted from the ICDAR2019-HDRC-IR test set comprise the training set.\nThe competition evaluated the test set for two tasks, retrieval per writer and per image.\nFor the writer task, the best system in terms of \\ac{mAP} used a ResNet \\cite{He2016DeepRL} with 20 layers trained on SIFT keypoints and multi-\\ac{VLAD} encoding, PCA for descriptor dimensionality reduction, k-means clustering on the descriptors, and cosine similarity for the final results.\nThe whole process was based on the work presented in \\cite{christlein2017unsupervised}.\nAccuracy, Pr@10, and Pr@100 metrics were used.\nThe system that achieved the highest values used a ResNet50 \\cite{He2016DeepRL} feature extractor with whole fragment image input and the $\\chi^2$ distance.\nThis system also obtained the highest values in all retrieval per image task metrics."
  },
  {
    "id": "2b100a23",
    "name": "ICDAR 2021 HDC",
    "statistics": "Font: 35k train, 5, 506 test images Script: CLaMM 2017 train, 1, 256 test images Date: 11, 294 train, 2, 516 test images Location: 5, 517 train, 60 val, 300 test images",
    "class": "Font: same as Section \\ref{sssec:fonts} without \"other font\" and \"not a font\" classes (10 classes) Script: same as Section \\ref{sssec:ICFHR16CLAMM} Date: date ranges Location: Cluny, Corbie, Citeaux, Florence Fonteney, Himanis, Milan, Paris, Signy MontSaintMichel, SaintBertin SaintGermainDesPres, SainMatrialDeLinoges",
    "task": [
      "Font classification",
      "Script classification",
      "Date prediction",
      "Location prediction"
    ],
    "language": [
      "Latin"
    ],
    "document_type": "Handwritten and printed page images in Latin",
    "mode": [
      "Color",
      "Grayscale"
    ],
    "resolution": "Various",
    "format": [
      "TIFF",
      "JPEG"
    ],
    "reference": "10.1145/3476887.3476913",
    "description": "\nThe competition on Historical Document Classification \\cite{10.1007/978-3-030-86337-1_41}, which was hosted at ICDAR 2021 included three tasks for single or multilabel document classification: font/script, location, and date.\nFor the first task of font/script classification, the organizers provided the multiple font dataset presented in Section \\ref{sssec:fonts} as the training dataset for the font classification task and the ICDAR17 (\\ref{sssec:ICDAR17CLAMM}) and ICDAR16 (\\ref{sssec:ICFHR16CLAMM}) CLaMM datasets for the script classification task.\nTwo new test sets were introduced for each task. \nFor the date classification tasks, a new training and test set containing 11,294 and 2,516 images, respectively, was introduced. \nThe ICDAR17 CLaMM dataset was suggested as an additional training set. \nNew training, validation, and test sets of French manuscript images with 13 location labels were introduced for the location task. \nThe competition results were evaluated using the top-1 accuracy on the test sets for the font/script and location tasks and the mean average error (MAE) for the date classification task. \nFor all tasks, the winning team used \\ac{CNN} operating either on non overlapping patches of four different scales or text lines acquired through segmentation \\cite{10.1007/978-3-031-06555-2_11}. \nFor the date classification task, instead of a cross-entropy loss, which was used in the other tasks, an interval regression loss was used that treated the task as a regression problem."
  },
  {
    "id": "75138526",
    "name": "IAM-HistDB Parzival",
    "statistics": "47 pages, 4477 lines, 23478 words",
    "class": "4, 934 word labels 93 letters, Handwritten",
    "task": [
      "Text recognition",
      "Word spotting",
      "Layout analysis"
    ],
    "language": [
      "Medieval German"
    ],
    "document_type": "Page images of a 13nth century manuscript written in Gothic script by 3 writers",
    "mode": [
      "Color",
      "Binary"
    ],
    "resolution": "300 dpi",
    "format": [
      "JPEG",
      "PNG"
    ],
    "reference": "5306020",
    "description": "\nIAM-HistDB \\cite{10.1145/1815330.1815331} is a highly used database of handwritten historical manuscript images that contains three datasets: Saint Gall, Parzival, and George Washington (GW).\nWe present these datasets in the following paragraphs.\n%All datasets contain Latin, Old German, and English text pages from the 9th, 13th and 18th century, respectively.\nThe Saint Gall database \\cite{10.1145/2037342.2037348} is a set of 60 page images and 1,410 binarized and normalized text-line images of manuscripts written in the \\nth{9} century in Latin language and Carolingian script by one writer.\nThe text edition for every page image was provided.\nThe pages are composed of 11,597 words, 4,890 word labels, 5,436 word spellings, and 49 letters.\nThe ground truth includes the line-level text transcriptions and the word and line locations.\nAn evaluation of a transcription alignment system based on HMM is proposed in the paper and compared with three more reference systems.\nThe Parzival database \\cite{FISCHER2012934} provides handwritten documents from the \\nth{13} century originating from three writers and was written in Old German and Gothic script.\nIt contains 47 pages, 4,477 text lines, 23,478 words, 4,934 word categories, and 93 letters.\nSimilar to St. Gall, the line and word images are binarized and normalized.\nAs ground truth, Parzival includes line- and word-level transcriptions.\nThe work presented in \\cite{5306020} used a HMM-based system similar to \\cite{10.5555/505741.505745} and the BLSTM introduced in \\cite{Graves2009855} recognizer for automatic handwriting recognition on the Parzival dataset and achieved a word \\ac{acc} of 88.69\\% and 93.32\\%.\nFurthermore, in \\cite{FISCHER2012934}, a lexicon-free word spotting method based on character HMMs was proposed and evaluated on the Parzival and GW datasets.\nThe GW database \\cite{FISCHER2012934} is comprised of \\nth{18} century documents from the George Washington Papers and contains 656 text and 4,894 word images, binarized and normalized, along with their transcription annotations.\nThe pages are written in English by two writers in longhand script.\nThe dataset statistics also include 1,471 word classes and 82 letters.\nThis dataset is widely used to evaluate word spotting algorithms. \n\\cite{5871643} used this database and compared a proposed word spotting method that used a BLSTM and a modified CTC algorithm with a HMM \\cite{10.1016/j.patcog.2009.02.005} and a DTW \\cite{Rath2006WordSF} method.\nThe paper presented average precision results using GW and Parzival datasets and the proposed method achieved 0.84 on the GW and 0.94 average precision on Parzival."
  },
  {
    "id": "db32a851",
    "name": "AMADI",
    "statistics": "100 pages, 100 binarized_images, 15000 word_patched 18000 characters",
    "class": "133 character classes",
    "task": [
      "Binarization",
      "Word spotting",
      "Character recognition"
    ],
    "language": [
      "Balinese"
    ],
    "document_type": "Palm leaf Balinese manuscripts",
    "mode": [
      "Color"
    ],
    "resolution": "N/A",
    "format": [
      "JPEG"
    ],
    "reference": "7814058",
    "description": "\nAMADI\\_LontarSet \\cite{7814058} is a collection of palm leaf manuscripts from Bali.\nThis dataset was a part of the ICFHR 2016 Competition on the Analysis of Handwritten Text in Images of Balinese Palm Leaf Manuscripts \\cite{7814130}.\nIt contains binarized, word annotated, and isolated character annotated ground truth images used for the following challenges: Binarization of Palm Leaf Manuscript Images, Query-by-Example Word Spotting on Palm Leaf Manuscript Images, and Isolated Character Recognition of Balinese Script in Palm Leaf Manuscript Images, respectively.\nFor Challenge 1, binarization, the dataset includes 50 training images, 100 binarized images from two different sources (50 and 50) as ground truth, and 50 test images.\nThe team that outperformed the others used a pretrained \\ac{FCN} on handwritten documents as presented in the work by Wolf et al. \\cite{1048482} that was fine-tuned on the DIBCO \\cite{5277767} and H-DIBCO \\cite{8583809} images and then fine-tuned on the competition images.\nThe results were evaluated according to the F-Measure (FM), PSNR, and Negative Rate Metric (NRM) between the ground truth and the predicted binarized images.\nFor Challenge 2, word-spotting, a split of 130 train and 100 test images was provided along with 15,022 word annotated patches for training. \nMoreover, 36 word annotated patches were given as query test.\nThe goal was to use a query word image patch to retrieve similar word image patches in palm leaf manuscripts; however, there were no submissions for this challenge.\nFinally, Challenge 3 aimed to recognize isolated Balinese characters distributed over 130 character classes.\nThe training set contains 11,710 labeled patch images, and the test set contains 7,673.\nThe method with the highest \\ac{RA} (VMQDF) initially preprocessed the input images by resizing, binarizing using the OTSU method, and then defeating grayscale variation.\nThen, synthetic samples were generated based on the preprocessed samples using the method in \\cite{Shao2012FastSV}, gradient features were extracted for all images.\nFinally, a classifier was trained on the new set that contained the original and \ngenerated images, while at the test phase, for every sample, 97 synthetic images were generated and treated according to the previously mentioned method."
  }
]