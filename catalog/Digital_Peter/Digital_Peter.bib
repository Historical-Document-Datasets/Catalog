@inbook{10.1145/3476887.3476892,
    author = "Mark, Potanin and Denis, Dimitrov and Alex, Shonenkov and Vladimir, Bataev and Denis, Karachev and Maxim, Novopoltsev and Andrey, Chertok",
    title = "Digital Peter: New Dataset, Competition and Handwriting Recognition Methods",
    year = "2021",
    isbn = "9781450386906",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    doi = "https://doi.org/10.1145/3476887.3476892",
    abstract = "This paper presents a new dataset of Peter the Great’s manuscripts and describes a segmentation procedure that converts initial images of documents into lines. This new dataset may be useful for researchers to train handwriting text recognition models as a benchmark when comparing different models. It consists of 9694 images and text files corresponding to different lines in historical documents. The open machine learning competition ”Digital Peter” was held based on the considered dataset. The baseline solution for this competition and advanced methods on handwritten text recognition are described in the article. The full dataset and all codes are publicly available.",
    booktitle = "The 6th International Workshop on Historical Document Imaging and Processing",
    pages = "43–48",
    numpages = "6"
}

@inproceedings{10.1145/1143844.1143891,
    author = {Graves, Alex and Fern\'{a}ndez, Santiago and Gomez, Faustino and Schmidhuber, J\"{u}rgen},
    title = "{Connectionist Temporal Classification: Labelling Unsegmented Sequence Data with Recurrent Neural Networks}",
    year = "2006",
    isbn = "1595933832",
    publisher = "Association for Computing Machinery",
    address = "New York, NY, USA",
    doi = "10.1145/1143844.1143891",
    abstract = "Many real-world sequence learning tasks require the prediction of sequences of labels from noisy, unsegmented input data. In speech recognition, for example, an acoustic signal is transcribed into words or sub-word units. Recurrent neural networks (RNNs) are powerful sequence learners that would seem well suited to such tasks. However, because they require pre-segmented training data, and post-processing to transform their outputs into label sequences, their applicability has so far been limited. This paper presents a novel method for training RNNs to label unsegmented sequences directly, thereby solving both problems. An experiment on the TIMIT speech corpus demonstrates its advantages over both a baseline HMM and a hybrid HMM-RNN.",
    booktitle = "Proceedings of the 23rd International Conference on Machine Learning",
    pages = "369–376",
    numpages = "8",
    location = "Pittsburgh, Pennsylvania, USA",
    series = "ICML '06"
}
